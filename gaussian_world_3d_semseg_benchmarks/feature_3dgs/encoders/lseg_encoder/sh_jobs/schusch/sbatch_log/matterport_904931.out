Namespace(model='encnet', backbone='clip_vitl16_384', dataset='ignore', workers=0, base_size=520, crop_size=480, train_split='train', aux=False, se_loss=False, se_weight=0.2, batch_size=16, test_batch_size=16, no_cuda=False, seed=1, weights='demo_e200.ckpt', eval=False, export=None, acc_bn=False, test_val=False, no_val=False, module='lseg', data_path=None, scale_inv=False, widehead=True, widehead_hr=False, ignore_index=-1, label_src='default', jobname='default', strict=True, arch_option=0, block_depth=0, activation='lrelu', outdir='/srv/beegfs02/scratch/qimaqi_data/data/scannet_full/data', test_rgb_dir='/usr/bmicnas02/data-biwi-01/qimaqi_data/workspace/iccv_2025/GS_Transformer/data/scannet_full/data/09c1414f1b/dslr/undistorted_images', resize_max=1.25, cuda=True)
/scratch_net/schusch/qimaqi/miniconda3/envs/feature_3dgs/lib/python3.9/site-packages/pytorch_lightning/utilities/migration/migration.py:208: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.
Lightning automatically upgraded your loaded checkpoint from v1.3.5 to v2.2.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint demo_e200.ckpt`
** Use norm [0.5, 0.5, 0.5], [0.5, 0.5, 0.5] as the mean and std **

  0%|                                               | 0.00/338M [00:00<?, ?iB/s]
  2%|▉                                     | 7.97M/338M [00:00<00:04, 83.5MiB/s]
  5%|█▊                                    | 15.9M/338M [00:00<00:04, 81.3MiB/s]
  7%|██▋                                   | 23.7M/338M [00:00<00:05, 55.3MiB/s]
  9%|███▎                                  | 29.6M/338M [00:00<00:05, 56.9MiB/s]
 11%|███▉                                  | 35.5M/338M [00:00<00:05, 56.7MiB/s]
 13%|████▉                                 | 43.6M/338M [00:00<00:04, 64.8MiB/s]
 16%|█████▉                                | 53.1M/338M [00:00<00:03, 75.4MiB/s]
 18%|██████▉                               | 61.4M/338M [00:00<00:03, 78.7MiB/s]
 20%|███████▊                              | 69.1M/338M [00:01<00:03, 78.4MiB/s]
 23%|████████▊                             | 78.8M/338M [00:01<00:03, 85.1MiB/s]
 26%|█████████▊                            | 87.0M/338M [00:01<00:03, 80.3MiB/s]
 28%|██████████▋                           | 94.8M/338M [00:01<00:03, 78.9MiB/s]
 31%|███████████▉                           | 103M/338M [00:01<00:03, 81.7MiB/s]
 33%|████████████▉                          | 112M/338M [00:01<00:02, 84.6MiB/s]
 36%|█████████████▉                         | 121M/338M [00:01<00:02, 85.5MiB/s]
 38%|██████████████▊                        | 129M/338M [00:01<00:02, 81.0MiB/s]
 41%|███████████████▊                       | 137M/338M [00:01<00:02, 82.8MiB/s]
 43%|████████████████▊                      | 145M/338M [00:01<00:02, 82.4MiB/s]
 45%|█████████████████▋                     | 153M/338M [00:02<00:02, 76.5MiB/s]
 48%|██████████████████▌                    | 160M/338M [00:02<00:02, 77.2MiB/s]
 50%|███████████████████▍                   | 168M/338M [00:02<00:02, 77.6MiB/s]
 52%|████████████████████▍                  | 177M/338M [00:02<00:02, 80.6MiB/s]
 55%|█████████████████████▎                 | 185M/338M [00:02<00:01, 81.7MiB/s]
 57%|██████████████████████▎                | 193M/338M [00:02<00:01, 81.1MiB/s]
 59%|███████████████████████▏               | 201M/338M [00:02<00:01, 81.2MiB/s]
 62%|████████████████████████▏              | 209M/338M [00:02<00:01, 83.4MiB/s]
 64%|█████████████████████████              | 217M/338M [00:02<00:01, 80.8MiB/s]
 67%|█████████████████████████▉             | 225M/338M [00:03<00:01, 78.3MiB/s]
 69%|██████████████████████████▉            | 233M/338M [00:03<00:01, 79.9MiB/s]
 71%|███████████████████████████▊           | 240M/338M [00:03<00:01, 76.0MiB/s]
 73%|████████████████████████████▋          | 248M/338M [00:03<00:01, 70.3MiB/s]
 75%|█████████████████████████████▍         | 255M/338M [00:03<00:01, 64.9MiB/s]
 77%|██████████████████████████████▏        | 261M/338M [00:03<00:01, 61.3MiB/s]
 80%|███████████████████████████████        | 269M/338M [00:03<00:01, 66.3MiB/s]
 82%|███████████████████████████████▉       | 277M/338M [00:03<00:00, 71.1MiB/s]
 84%|████████████████████████████████▉      | 285M/338M [00:03<00:00, 75.1MiB/s]
 87%|█████████████████████████████████▋     | 292M/338M [00:04<00:00, 72.4MiB/s]
 89%|██████████████████████████████████▌    | 299M/338M [00:04<00:00, 70.7MiB/s]
 91%|███████████████████████████████████▎   | 306M/338M [00:04<00:00, 65.2MiB/s]
 93%|████████████████████████████████████▏  | 313M/338M [00:04<00:00, 67.5MiB/s]
 95%|█████████████████████████████████████  | 321M/338M [00:04<00:00, 71.4MiB/s]
 97%|█████████████████████████████████████▉ | 328M/338M [00:04<00:00, 72.4MiB/s]
 99%|██████████████████████████████████████▊| 336M/338M [00:04<00:00, 74.9MiB/s]
100%|███████████████████████████████████████| 338M/338M [00:04<00:00, 74.9MiB/s]
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/jtcxE69GiFV_32/images
load /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/jtcxE69GiFV_32/images as image directroy for FolderLoader
LSegModule(
  (net): LSegNet(
    (clip_pretrained): CLIP(
      (visual): VisionTransformer(
        (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): Transformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (pretrained): Module(
      (model): VisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (pos_drop): Dropout(p=0.0, inplace=False)
        (patch_drop): Identity()
        (norm_pre): Identity()
        (blocks): Sequential(
          (0): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (1): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (2): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (3): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (4): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (5): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (6): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (7): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (8): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (9): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (10): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (11): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (12): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (13): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (14): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (15): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (16): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (17): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (18): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (19): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (20): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (21): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (22): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (23): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (fc_norm): Identity()
        (head_drop): Dropout(p=0.0, inplace=False)
        (head): Linear(in_features=1024, out_features=1000, bias=True)
      )
      (act_postprocess1): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
      )
      (act_postprocess2): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))
        (4): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))
      )
      (act_postprocess3): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (act_postprocess4): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
        (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
    (scratch): Module(
      (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (refinenet1): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet2): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet3): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet4): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (head1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
      (output_conv): Sequential(
        (0): Interpolate()
      )
    )
  )
)
scales [0.75, 1.0, 1.25, 1.75]
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/jtcxE69GiFV_32/images
outdir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/jtcxE69GiFV_32/rgb_feature_langseg
MultiEvalModule: base_size 520, crop_size 480

  0%|          | 0/36 [00:00<?, ?it/s]150
w, h = 320 256
scales [0.75, 1.0, 1.25, 1.75]
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/jtcxE69GiFV_32/images
outdir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/jtcxE69GiFV_32/rgb_feature_langseg
torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.3704116344451904
start make_pred
done makepred -0.010199546813964844
calculate PCA based on 1st image 12e9214fb1fc43f7aab15521c0260933_i0_0.jpg
PCA(n_components=3, random_state=42)
pca.explained_variance_ratio_ [0.583747386932373, 0.2317849099636078, 0.08155405521392822]
pca.singular_values_ [25.74016571044922, 16.219648361206055, 9.621031761169434]
-0.5474477648735047 0.12239085435867313
start save
1.1920928955078125e-06

  3%|▎         | 1/36 [00:14<08:15, 14.17s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.3701329231262207
start make_pred
done makepred -0.008015155792236328
start save
1.430511474609375e-06

  6%|▌         | 2/36 [00:21<05:38,  9.94s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.3823976516723633
start make_pred
done makepred -0.007980108261108398
start save
1.1920928955078125e-06

  8%|▊         | 3/36 [00:28<04:42,  8.55s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.3878800868988037
start make_pred
done makepred -0.007976770401000977
start save
7.152557373046875e-07

 11%|█         | 4/36 [00:34<04:10,  7.82s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.391927719116211
start make_pred
done makepred -0.00760960578918457
start save
9.5367431640625e-07

 14%|█▍        | 5/36 [00:41<03:51,  7.46s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.388887643814087
start make_pred
done makepred -0.007912158966064453
start save
1.1920928955078125e-06

 17%|█▋        | 6/36 [00:48<03:39,  7.32s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.3931407928466797
start make_pred
done makepred -0.00797128677368164
start save
1.1920928955078125e-06

 19%|█▉        | 7/36 [00:55<03:32,  7.32s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.394784688949585
start make_pred
done makepred -0.007961034774780273
start save
9.5367431640625e-07

 22%|██▏       | 8/36 [01:03<03:22,  7.25s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.3999602794647217
start make_pred
done makepred -0.00793910026550293
start save
2.1457672119140625e-06

 25%|██▌       | 9/36 [01:09<03:13,  7.15s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4024529457092285
start make_pred
done makepred -0.007574796676635742
start save
9.5367431640625e-07

 28%|██▊       | 10/36 [01:16<03:04,  7.09s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4030463695526123
start make_pred
done makepred -0.007597208023071289
start save
1.1920928955078125e-06

 31%|███       | 11/36 [01:23<02:55,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4040892124176025
start make_pred
done makepred -0.00792384147644043
start save
1.1920928955078125e-06

 33%|███▎      | 12/36 [01:30<02:45,  6.91s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.421584129333496
start make_pred
done makepred -0.007944345474243164
start save
9.5367431640625e-07

 36%|███▌      | 13/36 [01:37<02:39,  6.95s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.404867649078369
start make_pred
done makepred -0.007936239242553711
start save
1.1920928955078125e-06

 39%|███▉      | 14/36 [01:44<02:33,  6.98s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.406724214553833
start make_pred
done makepred -0.007976293563842773
start save
1.1920928955078125e-06

 42%|████▏     | 15/36 [01:51<02:26,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.408787965774536
start make_pred
done makepred -0.00786137580871582
start save
1.1920928955078125e-06

 44%|████▍     | 16/36 [01:58<02:19,  6.99s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4095706939697266
start make_pred
done makepred -0.007911205291748047
start save
1.430511474609375e-06

 47%|████▋     | 17/36 [02:05<02:13,  7.03s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4095969200134277
start make_pred
done makepred -0.007647275924682617
start save
9.5367431640625e-07

 50%|█████     | 18/36 [02:12<02:08,  7.12s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.407485246658325
start make_pred
done makepred -0.00795888900756836
start save
1.1920928955078125e-06

 53%|█████▎    | 19/36 [02:20<02:00,  7.10s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.5491838455200195
start make_pred
done makepred -0.007906436920166016
start save
1.1920928955078125e-06

 56%|█████▌    | 20/36 [02:27<01:54,  7.13s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.410290241241455
start make_pred
done makepred -0.007946014404296875
start save
7.152557373046875e-07

 58%|█████▊    | 21/36 [02:34<01:46,  7.13s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4086670875549316
start make_pred
done makepred -0.007953166961669922
start save
9.5367431640625e-07

 61%|██████    | 22/36 [02:41<01:39,  7.09s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412436008453369
start make_pred
done makepred -0.007586240768432617
start save
9.5367431640625e-07

 64%|██████▍   | 23/36 [02:48<01:32,  7.13s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4122262001037598
start make_pred
done makepred -0.007544994354248047
start save
9.5367431640625e-07

 67%|██████▋   | 24/36 [02:55<01:24,  7.08s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411630392074585
start make_pred
done makepred -0.007929801940917969
start save
1.430511474609375e-06

 69%|██████▉   | 25/36 [03:02<01:17,  7.05s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4116058349609375
start make_pred
done makepred -0.00795888900756836
start save
1.430511474609375e-06

 72%|███████▏  | 26/36 [03:09<01:11,  7.13s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4097602367401123
start make_pred
done makepred -0.007959365844726562
start save
9.5367431640625e-07

 75%|███████▌  | 27/36 [03:16<01:03,  7.08s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4084508419036865
start make_pred
done makepred -0.00793147087097168
start save
1.430511474609375e-06

 78%|███████▊  | 28/36 [03:23<00:56,  7.00s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4126272201538086
start make_pred
done makepred -0.007925748825073242
start save
1.1920928955078125e-06

 81%|████████  | 29/36 [03:30<00:49,  7.07s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4110825061798096
start make_pred
done makepred -0.007936716079711914
start save
1.1920928955078125e-06

 83%|████████▎ | 30/36 [03:38<00:42,  7.12s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413207769393921
start make_pred
done makepred -0.007435798645019531
start save
1.1920928955078125e-06

 86%|████████▌ | 31/36 [03:44<00:35,  7.04s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4097039699554443
start make_pred
done makepred -0.007933855056762695
start save
1.430511474609375e-06

 89%|████████▉ | 32/36 [03:51<00:27,  6.99s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.409259557723999
start make_pred
done makepred -0.007894277572631836
start save
1.1920928955078125e-06

 92%|█████████▏| 33/36 [03:58<00:20,  6.96s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4117302894592285
start make_pred
done makepred -0.00791168212890625
start save
1.1920928955078125e-06

 94%|█████████▍| 34/36 [04:05<00:13,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4133474826812744
start make_pred
done makepred -0.007624387741088867
start save
1.1920928955078125e-06

 97%|█████████▋| 35/36 [04:12<00:07,  7.02s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412879467010498
start make_pred
done makepred -0.007925748825073242
start save
1.430511474609375e-06

100%|██████████| 36/36 [04:19<00:00,  7.01s/it]
100%|██████████| 36/36 [04:19<00:00,  7.22s/it]
/scratch_net/schusch/qimaqi/miniconda3/envs/feature_3dgs/lib/python3.9/site-packages/pytorch_lightning/utilities/migration/migration.py:208: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.
Lightning automatically upgraded your loaded checkpoint from v1.3.5 to v2.2.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint demo_e200.ckpt`
** Use norm [0.5, 0.5, 0.5], [0.5, 0.5, 0.5] as the mean and std **
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/UwV83HsGsw3_22/images
load /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/UwV83HsGsw3_22/images as image directroy for FolderLoader
LSegModule(
  (net): LSegNet(
    (clip_pretrained): CLIP(
      (visual): VisionTransformer(
        (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): Transformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (pretrained): Module(
      (model): VisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (pos_drop): Dropout(p=0.0, inplace=False)
        (patch_drop): Identity()
        (norm_pre): Identity()
        (blocks): Sequential(
          (0): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (1): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (2): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (3): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (4): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (5): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (6): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (7): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (8): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (9): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (10): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (11): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (12): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (13): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (14): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (15): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (16): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (17): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (18): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (19): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (20): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (21): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (22): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (23): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (fc_norm): Identity()
        (head_drop): Dropout(p=0.0, inplace=False)
        (head): Linear(in_features=1024, out_features=1000, bias=True)
      )
      (act_postprocess1): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
      )
      (act_postprocess2): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))
        (4): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))
      )
      (act_postprocess3): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (act_postprocess4): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
        (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
    (scratch): Module(
      (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (refinenet1): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet2): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet3): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet4): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (head1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
      (output_conv): Sequential(
        (0): Interpolate()
      )
    )
  )
)
scales [0.75, 1.0, 1.25, 1.75]
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/UwV83HsGsw3_22/images
outdir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/UwV83HsGsw3_22/rgb_feature_langseg
MultiEvalModule: base_size 520, crop_size 480

  0%|          | 0/198 [00:00<?, ?it/s]150
w, h = 320 256
scales [0.75, 1.0, 1.25, 1.75]
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/UwV83HsGsw3_22/images
outdir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/UwV83HsGsw3_22/rgb_feature_langseg
torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.421342372894287
start make_pred
done makepred -0.00809931755065918
calculate PCA based on 1st image 0a5d5eef6d98406aa6e73cf7772bbc27_i0_0.jpg
PCA(n_components=3, random_state=42)
pca.explained_variance_ratio_ [0.9193922877311707, 0.05708106979727745, 0.012653491459786892]
pca.singular_values_ [53.9389533996582, 13.439964294433594, 6.3278656005859375]
-0.22700710594654083 0.5634245872497559
start save
1.1920928955078125e-06

  1%|          | 1/198 [00:07<23:48,  7.25s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.408095359802246
start make_pred
done makepred -0.007928133010864258
start save
9.5367431640625e-07

  1%|          | 2/198 [00:14<23:06,  7.07s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4093499183654785
start make_pred
done makepred -0.007484912872314453
start save
4.76837158203125e-07

  2%|▏         | 3/198 [00:21<22:45,  7.00s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.405632972717285
start make_pred
done makepred -0.007928133010864258
start save
4.76837158203125e-07

  2%|▏         | 4/198 [00:28<22:50,  7.06s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4084696769714355
start make_pred
done makepred -0.007938623428344727
start save
4.76837158203125e-07

  3%|▎         | 5/198 [00:35<22:41,  7.06s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4106268882751465
start make_pred
done makepred -0.007929086685180664
start save
4.76837158203125e-07

  3%|▎         | 6/198 [00:42<22:34,  7.06s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.41084361076355
start make_pred
done makepred -0.007963180541992188
start save
4.76837158203125e-07

  4%|▎         | 7/198 [00:49<22:17,  7.00s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4081151485443115
start make_pred
done makepred -0.0075397491455078125
start save
4.76837158203125e-07

  4%|▍         | 8/198 [00:56<22:11,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413409471511841
start make_pred
done makepred -0.007600545883178711
start save
7.152557373046875e-07

  5%|▍         | 9/198 [01:03<22:09,  7.03s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4126358032226562
start make_pred
done makepred -0.007938623428344727
start save
4.76837158203125e-07

  5%|▌         | 10/198 [01:10<22:16,  7.11s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413989782333374
start make_pred
done makepred -0.007930517196655273
start save
2.6226043701171875e-06

  6%|▌         | 11/198 [01:17<22:09,  7.11s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4099996089935303
start make_pred
done makepred -0.007908105850219727
start save
4.76837158203125e-07

  6%|▌         | 12/198 [01:24<21:48,  7.03s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4127097129821777
start make_pred
done makepred -0.007916688919067383
start save
9.5367431640625e-07

  7%|▋         | 13/198 [01:31<21:40,  7.03s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4108376502990723
start make_pred
done makepred -0.007921695709228516
start save
4.76837158203125e-07

  7%|▋         | 14/198 [01:38<21:30,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4124579429626465
start make_pred
done makepred -0.007960319519042969
start save
7.152557373046875e-07

  8%|▊         | 15/198 [01:45<21:20,  7.00s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4111809730529785
start make_pred
done makepred -0.007524728775024414
start save
4.76837158203125e-07

  8%|▊         | 16/198 [01:52<20:51,  6.88s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4272959232330322
start make_pred
done makepred -0.007939815521240234
start save
9.5367431640625e-07

  9%|▊         | 17/198 [01:59<20:42,  6.86s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4128875732421875
start make_pred
done makepred -0.007972955703735352
start save
7.152557373046875e-07

  9%|▉         | 18/198 [02:05<20:42,  6.90s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4133198261260986
start make_pred
done makepred -0.007979393005371094
start save
2.384185791015625e-07

 10%|▉         | 19/198 [02:12<20:36,  6.91s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.5458409786224365
start make_pred
done makepred -0.00793004035949707
start save
9.5367431640625e-07

 10%|█         | 20/198 [02:20<20:50,  7.02s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4131243228912354
start make_pred
done makepred -0.007470607757568359
start save
1.1920928955078125e-06

 11%|█         | 21/198 [02:27<20:41,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412217855453491
start make_pred
done makepred -0.007529735565185547
start save
1.430511474609375e-06

 11%|█         | 22/198 [02:34<20:42,  7.06s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4111311435699463
start make_pred
done makepred -0.00796365737915039
start save
4.76837158203125e-07

 12%|█▏        | 23/198 [02:41<20:29,  7.03s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.410811424255371
start make_pred
done makepred -0.007917404174804688
start save
4.76837158203125e-07

 12%|█▏        | 24/198 [02:48<20:18,  7.00s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.410792112350464
start make_pred
done makepred -0.00792694091796875
start save
4.76837158203125e-07

 13%|█▎        | 25/198 [02:55<20:00,  6.94s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412593364715576
start make_pred
done makepred -0.007894515991210938
start save
1.430511474609375e-06

 13%|█▎        | 26/198 [03:02<19:57,  6.96s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.41232967376709
start make_pred
done makepred -0.007976055145263672
start save
1.1920928955078125e-06

 14%|█▎        | 27/198 [03:08<19:38,  6.89s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4137918949127197
start make_pred
done makepred -0.007936954498291016
start save
1.430511474609375e-06

 14%|█▍        | 28/198 [03:15<19:33,  6.90s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412586212158203
start make_pred
done makepred -0.007932901382446289
start save
9.5367431640625e-07

 15%|█▍        | 29/198 [03:22<19:37,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4113268852233887
start make_pred
done makepred -0.00750422477722168
start save
9.5367431640625e-07

 15%|█▌        | 30/198 [03:29<19:15,  6.88s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4131762981414795
start make_pred
done makepred -0.007433891296386719
start save
9.5367431640625e-07

 16%|█▌        | 31/198 [03:36<19:01,  6.84s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4131155014038086
start make_pred
done makepred -0.007916927337646484
start save
9.5367431640625e-07

 16%|█▌        | 32/198 [03:43<19:18,  6.98s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4123573303222656
start make_pred
done makepred -0.00793766975402832
start save
2.384185791015625e-07

 17%|█▋        | 33/198 [03:50<19:11,  6.98s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4121272563934326
start make_pred
done makepred -0.007952451705932617
start save
7.152557373046875e-07

 17%|█▋        | 34/198 [03:57<19:14,  7.04s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412820816040039
start make_pred
done makepred -0.007923126220703125
start save
1.430511474609375e-06

 18%|█▊        | 35/198 [04:04<18:56,  6.98s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4067184925079346
start make_pred
done makepred -0.00792074203491211
start save
4.76837158203125e-07

 18%|█▊        | 36/198 [04:11<18:45,  6.95s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4132087230682373
start make_pred
done makepred -0.007958173751831055
start save
1.1920928955078125e-06

 19%|█▊        | 37/198 [04:18<18:34,  6.92s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.41434907913208
start make_pred
done makepred -0.007585048675537109
start save
1.1920928955078125e-06

 19%|█▉        | 38/198 [04:25<18:39,  7.00s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413367509841919
start make_pred
done makepred -0.007889032363891602
start save
7.152557373046875e-07

 20%|█▉        | 39/198 [04:32<18:45,  7.08s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4131226539611816
start make_pred
done makepred -0.00792694091796875
start save
7.152557373046875e-07

 20%|██        | 40/198 [04:39<18:27,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4114534854888916
start make_pred
done makepred -0.007886409759521484
start save
4.76837158203125e-07

 21%|██        | 41/198 [04:46<18:16,  6.98s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411766290664673
start make_pred
done makepred -0.007885932922363281
start save
4.76837158203125e-07

 21%|██        | 42/198 [04:53<18:16,  7.03s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.410348892211914
start make_pred
done makepred -0.007519960403442383
start save
4.76837158203125e-07

 22%|██▏       | 43/198 [05:00<17:59,  6.96s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4263057708740234
start make_pred
done makepred -0.0076007843017578125
start save
9.5367431640625e-07

 22%|██▏       | 44/198 [05:07<17:51,  6.96s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4118924140930176
start make_pred
done makepred -0.00790548324584961
start save
1.430511474609375e-06

 23%|██▎       | 45/198 [05:14<17:38,  6.92s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.415771245956421
start make_pred
done makepred -0.007895231246948242
start save
1.430511474609375e-06

 23%|██▎       | 46/198 [05:21<17:45,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.410682201385498
start make_pred
done makepred -0.007603883743286133
start save
1.1920928955078125e-06

 24%|██▎       | 47/198 [05:28<17:36,  7.00s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4115962982177734
start make_pred
done makepred -0.007906913757324219
start save
1.1920928955078125e-06

 24%|██▍       | 48/198 [05:35<17:37,  7.05s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413041591644287
start make_pred
done makepred -0.007887840270996094
start save
1.1920928955078125e-06

 25%|██▍       | 49/198 [05:42<17:30,  7.05s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.414400100708008
start make_pred
done makepred -0.007921934127807617
start save
7.152557373046875e-07

 25%|██▌       | 50/198 [05:49<17:04,  6.92s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413668155670166
start make_pred
done makepred -0.007890462875366211
start save
4.76837158203125e-07

 26%|██▌       | 51/198 [05:56<16:53,  6.89s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4140002727508545
start make_pred
done makepred -0.007601022720336914
start save
9.5367431640625e-07

 26%|██▋       | 52/198 [06:02<16:42,  6.87s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.414602518081665
start make_pred
done makepred -0.007607698440551758
start save
4.76837158203125e-07

 27%|██▋       | 53/198 [06:10<16:50,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412585496902466
start make_pred
done makepred -0.007911920547485352
start save
1.430511474609375e-06

 27%|██▋       | 54/198 [06:17<16:41,  6.96s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4067001342773438
start make_pred
done makepred -0.007937908172607422
start save
1.1920928955078125e-06

 28%|██▊       | 55/198 [06:23<16:16,  6.83s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.428333282470703
start make_pred
done makepred -0.007905721664428711
start save
4.76837158203125e-07

 28%|██▊       | 56/198 [06:30<16:25,  6.94s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.554687976837158
start make_pred
done makepred -0.007897377014160156
start save
7.152557373046875e-07

 29%|██▉       | 57/198 [06:37<16:22,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4089131355285645
start make_pred
done makepred -0.007901906967163086
start save
4.76837158203125e-07

 29%|██▉       | 58/198 [06:44<16:12,  6.95s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4134085178375244
start make_pred
done makepred -0.007892608642578125
start save
4.76837158203125e-07

 30%|██▉       | 59/198 [06:51<16:03,  6.93s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4106318950653076
start make_pred
done makepred -0.007636070251464844
start save
4.76837158203125e-07

 30%|███       | 60/198 [06:58<16:02,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413919448852539
start make_pred
done makepred -0.007925748825073242
start save
7.152557373046875e-07

 31%|███       | 61/198 [07:06<16:14,  7.11s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.410997152328491
start make_pred
done makepred -0.007888317108154297
start save
9.5367431640625e-07

 31%|███▏      | 62/198 [07:13<16:04,  7.09s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4115560054779053
start make_pred
done makepred -0.007903099060058594
start save
1.1920928955078125e-06

 32%|███▏      | 63/198 [07:20<15:53,  7.06s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4105660915374756
start make_pred
done makepred -0.00792241096496582
start save
1.1920928955078125e-06

 32%|███▏      | 64/198 [07:27<15:48,  7.08s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.414185047149658
start make_pred
done makepred -0.0074574947357177734
start save
1.430511474609375e-06

 33%|███▎      | 65/198 [07:34<15:35,  7.03s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413400411605835
start make_pred
done makepred -0.007543087005615234
start save
9.5367431640625e-07

 33%|███▎      | 66/198 [07:41<15:22,  6.99s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.410076141357422
start make_pred
done makepred -0.007905244827270508
start save
7.152557373046875e-07

 34%|███▍      | 67/198 [07:48<15:16,  7.00s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4131827354431152
start make_pred
done makepred -0.00792241096496582
start save
2.384185791015625e-07

 34%|███▍      | 68/198 [07:55<15:11,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4125559329986572
start make_pred
done makepred -0.007503986358642578
start save
4.76837158203125e-07

 35%|███▍      | 69/198 [08:02<15:01,  6.99s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.409665584564209
start make_pred
done makepred -0.007946014404296875
start save
7.152557373046875e-07

 35%|███▌      | 70/198 [08:08<14:52,  6.98s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.409823179244995
start make_pred
done makepred -0.007930755615234375
start save
9.5367431640625e-07

 36%|███▌      | 71/198 [08:15<14:44,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.416217803955078
start make_pred
done makepred -0.007842540740966797
start save
4.76837158203125e-07

 36%|███▋      | 72/198 [08:22<14:39,  6.98s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.410001754760742
start make_pred
done makepred -0.007866144180297852
start save
7.152557373046875e-07

 37%|███▋      | 73/198 [08:29<14:33,  6.99s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.410846710205078
start make_pred
done makepred -0.007372379302978516
start save
4.76837158203125e-07

 37%|███▋      | 74/198 [08:37<14:28,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411602735519409
start make_pred
done makepred -0.0074579715728759766
start save
7.152557373046875e-07

 38%|███▊      | 75/198 [08:43<14:19,  6.99s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4082751274108887
start make_pred
done makepred -0.007891654968261719
start save
9.5367431640625e-07

 38%|███▊      | 76/198 [08:51<14:15,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.5445873737335205
start make_pred
done makepred -0.007833719253540039
start save
1.1920928955078125e-06

 39%|███▉      | 77/198 [08:58<14:36,  7.24s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.410703182220459
start make_pred
done makepred -0.007840871810913086
start save
1.1920928955078125e-06

 39%|███▉      | 78/198 [09:05<14:14,  7.12s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412794828414917
start make_pred
done makepred -0.007873296737670898
start save
7.152557373046875e-07

 40%|███▉      | 79/198 [09:12<13:57,  7.04s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4092915058135986
start make_pred
done makepred -0.007815361022949219
start save
1.1920928955078125e-06

 40%|████      | 80/198 [09:19<13:53,  7.06s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.414926052093506
start make_pred
done makepred -0.007827520370483398
start save
1.1920928955078125e-06

 41%|████      | 81/198 [09:26<13:42,  7.03s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411324977874756
start make_pred
done makepred -0.007430315017700195
start save
9.5367431640625e-07

 41%|████▏     | 82/198 [09:33<13:24,  6.94s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4260571002960205
start make_pred
done makepred -0.007873296737670898
start save
1.430511474609375e-06

 42%|████▏     | 83/198 [09:40<13:19,  6.95s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411912679672241
start make_pred
done makepred -0.007786989212036133
start save
7.152557373046875e-07

 42%|████▏     | 84/198 [09:47<13:13,  6.96s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4146780967712402
start make_pred
done makepred -0.007796049118041992
start save
1.1920928955078125e-06

 43%|████▎     | 85/198 [09:54<13:09,  6.99s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.415743589401245
start make_pred
done makepred -0.0078277587890625
start save
4.76837158203125e-07

 43%|████▎     | 86/198 [10:01<12:57,  6.94s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4094231128692627
start make_pred
done makepred -0.007394552230834961
start save
4.76837158203125e-07

 44%|████▍     | 87/198 [10:07<12:46,  6.90s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.410443067550659
start make_pred
done makepred -0.00738835334777832
start save
7.152557373046875e-07

 44%|████▍     | 88/198 [10:15<12:45,  6.96s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4110636711120605
start make_pred
done makepred -0.007842302322387695
start save
7.152557373046875e-07

 45%|████▍     | 89/198 [10:21<12:29,  6.87s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4283623695373535
start make_pred
done makepred -0.0078067779541015625
start save
4.76837158203125e-07

 45%|████▌     | 90/198 [10:28<12:18,  6.84s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4273526668548584
start make_pred
done makepred -0.007439851760864258
start save
1.1920928955078125e-06

 46%|████▌     | 91/198 [10:35<12:02,  6.75s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4276680946350098
start make_pred
done makepred -0.007805585861206055
start save
4.76837158203125e-07

 46%|████▋     | 92/198 [10:41<11:59,  6.78s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.407498598098755
start make_pred
done makepred -0.007863998413085938
start save
7.152557373046875e-07

 47%|████▋     | 93/198 [10:48<11:54,  6.81s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.408090591430664
start make_pred
done makepred -0.007421970367431641
start save
4.76837158203125e-07

 47%|████▋     | 94/198 [10:55<11:50,  6.83s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4124393463134766
start make_pred
done makepred -0.00781559944152832
start save
7.152557373046875e-07

 48%|████▊     | 95/198 [11:02<11:48,  6.87s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.539262294769287
start make_pred
done makepred -0.007426023483276367
start save
4.76837158203125e-07

 48%|████▊     | 96/198 [11:09<11:52,  6.99s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.41550612449646
start make_pred
done makepred -0.00745081901550293
start save
1.1920928955078125e-06

 49%|████▉     | 97/198 [11:17<11:53,  7.06s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411724090576172
start make_pred
done makepred -0.007851839065551758
start save
4.76837158203125e-07

 49%|████▉     | 98/198 [11:23<11:40,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.414982557296753
start make_pred
done makepred -0.007857322692871094
start save
9.5367431640625e-07

 50%|█████     | 99/198 [11:31<11:38,  7.05s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412476062774658
start make_pred
done makepred -0.007809162139892578
start save
7.152557373046875e-07

 51%|█████     | 100/198 [11:38<11:28,  7.02s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.410733461380005
start make_pred
done makepred -0.007868528366088867
start save
1.1920928955078125e-06

 51%|█████     | 101/198 [11:45<11:25,  7.07s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4109506607055664
start make_pred
done makepred -0.00787663459777832
start save
7.152557373046875e-07

 52%|█████▏    | 102/198 [11:52<11:20,  7.08s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411456823348999
start make_pred
done makepred -0.0078067779541015625
start save
4.76837158203125e-07

 52%|█████▏    | 103/198 [11:59<11:12,  7.08s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.41318678855896
start make_pred
done makepred -0.007473468780517578
start save
4.76837158203125e-07

 53%|█████▎    | 104/198 [12:06<11:08,  7.11s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411451816558838
start make_pred
done makepred -0.007861137390136719
start save
1.430511474609375e-06

 53%|█████▎    | 105/198 [12:13<11:06,  7.17s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4109842777252197
start make_pred
done makepred -0.00783848762512207
start save
9.5367431640625e-07

 54%|█████▎    | 106/198 [12:21<11:00,  7.18s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4128258228302
start make_pred
done makepred -0.007862567901611328
start save
9.5367431640625e-07

 54%|█████▍    | 107/198 [12:28<10:45,  7.09s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4116029739379883
start make_pred
done makepred -0.00791025161743164
start save
9.5367431640625e-07

 55%|█████▍    | 108/198 [12:34<10:33,  7.04s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411843776702881
start make_pred
done makepred -0.007495880126953125
start save
7.152557373046875e-07

 55%|█████▌    | 109/198 [12:41<10:20,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4110403060913086
start make_pred
done makepred -0.007512569427490234
start save
7.152557373046875e-07

 56%|█████▌    | 110/198 [12:48<10:15,  6.99s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413372039794922
start make_pred
done makepred -0.007865428924560547
start save
4.76837158203125e-07

 56%|█████▌    | 111/198 [12:55<10:01,  6.91s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4132795333862305
start make_pred
done makepred -0.007871150970458984
start save
4.76837158203125e-07

 57%|█████▋    | 112/198 [13:02<10:04,  7.03s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4137444496154785
start make_pred
done makepred -0.008121728897094727
start save
7.152557373046875e-07

 57%|█████▋    | 113/198 [13:09<09:59,  7.05s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.414224624633789
start make_pred
done makepred -0.007516145706176758
start save
7.152557373046875e-07

 58%|█████▊    | 114/198 [13:16<09:45,  6.98s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4071476459503174
start make_pred
done makepred -0.007527589797973633
start save
4.76837158203125e-07

 58%|█████▊    | 115/198 [13:23<09:37,  6.96s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411961793899536
start make_pred
done makepred -0.007898092269897461
start save
9.5367431640625e-07

 59%|█████▊    | 116/198 [13:30<09:35,  7.02s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411778450012207
start make_pred
done makepred -0.007880926132202148
start save
7.152557373046875e-07

 59%|█████▉    | 117/198 [13:37<09:20,  6.93s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.427036762237549
start make_pred
done makepred -0.007486581802368164
start save
7.152557373046875e-07

 60%|█████▉    | 118/198 [13:44<09:17,  6.96s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412532329559326
start make_pred
done makepred -0.007436513900756836
start save
9.5367431640625e-07

 60%|██████    | 119/198 [13:51<09:10,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413645029067993
start make_pred
done makepred -0.007862091064453125
start save
4.76837158203125e-07

 61%|██████    | 120/198 [13:58<09:01,  6.94s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411440849304199
start make_pred
done makepred -0.007820367813110352
start save
4.76837158203125e-07

 61%|██████    | 121/198 [14:05<08:57,  6.98s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.410598039627075
start make_pred
done makepred -0.007817745208740234
start save
7.152557373046875e-07

 62%|██████▏   | 122/198 [14:12<08:41,  6.87s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413134813308716
start make_pred
done makepred -0.007840156555175781
start save
2.384185791015625e-07

 62%|██████▏   | 123/198 [14:19<08:43,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411207675933838
start make_pred
done makepred -0.0078067779541015625
start save
4.76837158203125e-07

 63%|██████▎   | 124/198 [14:26<08:45,  7.10s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4135637283325195
start make_pred
done makepred -0.007428169250488281
start save
4.76837158203125e-07

 63%|██████▎   | 125/198 [14:33<08:36,  7.07s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4131827354431152
start make_pred
done makepred -0.007234811782836914
start save
7.152557373046875e-07

 64%|██████▎   | 126/198 [14:40<08:27,  7.04s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.410505771636963
start make_pred
done makepred -0.0077991485595703125
start save
4.76837158203125e-07

 64%|██████▍   | 127/198 [14:47<08:22,  7.08s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4112749099731445
start make_pred
done makepred -0.007803440093994141
start save
1.1920928955078125e-06

 65%|██████▍   | 128/198 [14:54<08:12,  7.04s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4120137691497803
start make_pred
done makepred -0.007846355438232422
start save
1.1920928955078125e-06

 65%|██████▌   | 129/198 [15:01<08:00,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412816047668457
start make_pred
done makepred -0.007819414138793945
start save
7.152557373046875e-07

 66%|██████▌   | 130/198 [15:08<07:51,  6.93s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4090142250061035
start make_pred
done makepred -0.007498025894165039
start save
7.152557373046875e-07

 66%|██████▌   | 131/198 [15:15<07:40,  6.88s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4116547107696533
start make_pred
done makepred -0.007468223571777344
start save
4.76837158203125e-07

 67%|██████▋   | 132/198 [15:22<07:35,  6.89s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4140446186065674
start make_pred
done makepred -0.007866144180297852
start save
4.76837158203125e-07

 67%|██████▋   | 133/198 [15:29<07:32,  6.96s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412285327911377
start make_pred
done makepred -0.00782012939453125
start save
4.76837158203125e-07

 68%|██████▊   | 134/198 [15:36<07:22,  6.92s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4161133766174316
start make_pred
done makepred -0.007091045379638672
start save
1.1920928955078125e-06

 68%|██████▊   | 135/198 [15:42<07:14,  6.90s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413311243057251
start make_pred
done makepred -0.00790548324584961
start save
1.430511474609375e-06

 69%|██████▊   | 136/198 [15:49<07:06,  6.88s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412752389907837
start make_pred
done makepred -0.007871627807617188
start save
4.76837158203125e-07

 69%|██████▉   | 137/198 [15:56<07:00,  6.89s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4128880500793457
start make_pred
done makepred -0.007909297943115234
start save
7.152557373046875e-07

 70%|██████▉   | 138/198 [16:03<06:48,  6.81s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4303834438323975
start make_pred
done makepred -0.007853269577026367
start save
4.76837158203125e-07

 70%|███████   | 139/198 [16:10<06:50,  6.96s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411543846130371
start make_pred
done makepred -0.007504701614379883
start save
1.1920928955078125e-06

 71%|███████   | 140/198 [16:17<06:47,  7.02s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412574052810669
start make_pred
done makepred -0.00748753547668457
start save
9.5367431640625e-07

 71%|███████   | 141/198 [16:24<06:39,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4126036167144775
start make_pred
done makepred -0.007828474044799805
start save
9.5367431640625e-07

 72%|███████▏  | 142/198 [16:31<06:30,  6.98s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4093825817108154
start make_pred
done makepred -0.007858753204345703
start save
1.430511474609375e-06

 72%|███████▏  | 143/198 [16:38<06:20,  6.92s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413940668106079
start make_pred
done makepred -0.00783991813659668
start save
7.152557373046875e-07

 73%|███████▎  | 144/198 [16:45<06:12,  6.91s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411247491836548
start make_pred
done makepred -0.0074536800384521484
start save
4.76837158203125e-07

 73%|███████▎  | 145/198 [16:52<06:06,  6.91s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.408278226852417
start make_pred
done makepred -0.007809162139892578
start save
7.152557373046875e-07

 74%|███████▎  | 146/198 [16:59<05:58,  6.89s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.409360408782959
start make_pred
done makepred -0.007805824279785156
start save
4.76837158203125e-07

 74%|███████▍  | 147/198 [17:05<05:52,  6.90s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413198947906494
start make_pred
done makepred -0.00753021240234375
start save
7.152557373046875e-07

 75%|███████▍  | 148/198 [17:12<05:45,  6.92s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4112191200256348
start make_pred
done makepred -0.007823467254638672
start save
4.76837158203125e-07

 75%|███████▌  | 149/198 [17:19<05:39,  6.93s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4119009971618652
start make_pred
done makepred -0.007850885391235352
start save
4.76837158203125e-07

 76%|███████▌  | 150/198 [17:26<05:33,  6.94s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4157626628875732
start make_pred
done makepred -0.007807731628417969
start save
1.430511474609375e-06

 76%|███████▋  | 151/198 [17:33<05:27,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.415449619293213
start make_pred
done makepred -0.007834196090698242
start save
9.5367431640625e-07

 77%|███████▋  | 152/198 [17:41<05:22,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4141972064971924
start make_pred
done makepred -0.007856607437133789
start save
4.76837158203125e-07

 77%|███████▋  | 153/198 [17:47<05:14,  7.00s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4129791259765625
start make_pred
done makepred -0.007831096649169922
start save
1.1920928955078125e-06

 78%|███████▊  | 154/198 [17:54<05:08,  7.00s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4114787578582764
start make_pred
done makepred -0.007436275482177734
start save
1.1920928955078125e-06

 78%|███████▊  | 155/198 [18:01<04:59,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4144370555877686
start make_pred
done makepred -0.007906436920166016
start save
1.430511474609375e-06

 79%|███████▉  | 156/198 [18:08<04:52,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413224458694458
start make_pred
done makepred -0.007473945617675781
start save
4.76837158203125e-07

 79%|███████▉  | 157/198 [18:15<04:43,  6.92s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4087932109832764
start make_pred
done makepred -0.007874488830566406
start save
9.5367431640625e-07

 80%|███████▉  | 158/198 [18:22<04:37,  6.93s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4136974811553955
start make_pred
done makepred -0.007837533950805664
start save
9.5367431640625e-07

 80%|████████  | 159/198 [18:29<04:30,  6.95s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4090094566345215
start make_pred
done makepred -0.007829427719116211
start save
4.76837158203125e-07

 81%|████████  | 160/198 [18:36<04:21,  6.89s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412553310394287
start make_pred
done makepred -0.007870674133300781
start save
9.5367431640625e-07

 81%|████████▏ | 161/198 [18:43<04:16,  6.93s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.414415121078491
start make_pred
done makepred -0.007489681243896484
start save
7.152557373046875e-07

 82%|████████▏ | 162/198 [18:50<04:10,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4139270782470703
start make_pred
done makepred -0.007395505905151367
start save
1.1920928955078125e-06

 82%|████████▏ | 163/198 [18:57<04:05,  7.02s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4112472534179688
start make_pred
done makepred -0.007843017578125
start save
9.5367431640625e-07

 83%|████████▎ | 164/198 [19:04<04:00,  7.08s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4123244285583496
start make_pred
done makepred -0.007765769958496094
start save
7.152557373046875e-07

 83%|████████▎ | 165/198 [19:11<03:49,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4109561443328857
start make_pred
done makepred -0.007864713668823242
start save
4.76837158203125e-07

 84%|████████▍ | 166/198 [19:18<03:41,  6.91s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4145147800445557
start make_pred
done makepred -0.007850408554077148
start save
7.152557373046875e-07

 84%|████████▍ | 167/198 [19:25<03:33,  6.87s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4070041179656982
start make_pred
done makepred -0.007873058319091797
start save
7.152557373046875e-07

 85%|████████▍ | 168/198 [19:31<03:24,  6.83s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412846565246582
start make_pred
done makepred -0.00783085823059082
start save
4.76837158203125e-07

 85%|████████▌ | 169/198 [19:39<03:22,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4105923175811768
start make_pred
done makepred -0.007875442504882812
start save
1.1920928955078125e-06

 86%|████████▌ | 170/198 [19:46<03:15,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4125871658325195
start make_pred
done makepred -0.007297992706298828
start save
9.5367431640625e-07

 86%|████████▋ | 171/198 [19:53<03:11,  7.09s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4127092361450195
start make_pred
done makepred -0.007466793060302734
start save
7.152557373046875e-07

 87%|████████▋ | 172/198 [20:00<03:05,  7.13s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.414363384246826
start make_pred
done makepred -0.007841825485229492
start save
7.152557373046875e-07

 87%|████████▋ | 173/198 [20:07<02:56,  7.07s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4137659072875977
start make_pred
done makepred -0.007895231246948242
start save
1.1920928955078125e-06

 88%|████████▊ | 174/198 [20:14<02:50,  7.11s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4140570163726807
start make_pred
done makepred -0.007862329483032227
start save
4.76837158203125e-07

 88%|████████▊ | 175/198 [20:21<02:40,  7.00s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.426978588104248
start make_pred
done makepred -0.007905006408691406
start save
4.76837158203125e-07

 89%|████████▉ | 176/198 [20:28<02:36,  7.11s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4123260974884033
start make_pred
done makepred -0.007832527160644531
start save
4.76837158203125e-07

 89%|████████▉ | 177/198 [20:35<02:29,  7.11s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.5497777462005615
start make_pred
done makepred -0.007748603820800781
start save
4.76837158203125e-07

 90%|████████▉ | 178/198 [20:43<02:23,  7.15s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4123120307922363
start make_pred
done makepred -0.007843017578125
start save
4.76837158203125e-07

 90%|█████████ | 179/198 [20:50<02:15,  7.11s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4119532108306885
start make_pred
done makepred -0.007875204086303711
start save
4.76837158203125e-07

 91%|█████████ | 180/198 [20:57<02:08,  7.12s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4100937843322754
start make_pred
done makepred -0.007845163345336914
start save
4.76837158203125e-07

 91%|█████████▏| 181/198 [21:04<02:00,  7.08s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4103477001190186
start make_pred
done makepred -0.007898569107055664
start save
7.152557373046875e-07

 92%|█████████▏| 182/198 [21:11<01:52,  7.02s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4112205505371094
start make_pred
done makepred -0.007844209671020508
start save
9.5367431640625e-07

 92%|█████████▏| 183/198 [21:17<01:44,  6.94s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4113752841949463
start make_pred
done makepred -0.007390499114990234
start save
7.152557373046875e-07

 93%|█████████▎| 184/198 [21:25<01:38,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4131031036376953
start make_pred
done makepred -0.00744318962097168
start save
7.152557373046875e-07

 93%|█████████▎| 185/198 [21:32<01:31,  7.04s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4111218452453613
start make_pred
done makepred -0.007861852645874023
start save
7.152557373046875e-07

 94%|█████████▍| 186/198 [21:39<01:23,  6.99s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.414487838745117
start make_pred
done makepred -0.007887601852416992
start save
9.5367431640625e-07

 94%|█████████▍| 187/198 [21:46<01:17,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.410763740539551
start make_pred
done makepred -0.00784158706665039
start save
1.430511474609375e-06

 95%|█████████▍| 188/198 [21:53<01:10,  7.06s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4118080139160156
start make_pred
done makepred -0.007822751998901367
start save
1.1920928955078125e-06

 95%|█████████▌| 189/198 [22:00<01:03,  7.04s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411508798599243
start make_pred
done makepred -0.00786137580871582
start save
4.76837158203125e-07

 96%|█████████▌| 190/198 [22:07<00:56,  7.05s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412520408630371
start make_pred
done makepred -0.00782155990600586
start save
4.76837158203125e-07

 96%|█████████▋| 191/198 [22:14<00:48,  6.94s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412532329559326
start make_pred
done makepred -0.007443666458129883
start save
9.5367431640625e-07

 97%|█████████▋| 192/198 [22:21<00:42,  7.02s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4134223461151123
start make_pred
done makepred -0.007864236831665039
start save
9.5367431640625e-07

 97%|█████████▋| 193/198 [22:28<00:34,  6.94s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.429028034210205
start make_pred
done makepred -0.00783991813659668
start save
9.5367431640625e-07

 98%|█████████▊| 194/198 [22:35<00:27,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413085460662842
start make_pred
done makepred -0.00780797004699707
start save
1.1920928955078125e-06

 98%|█████████▊| 195/198 [22:42<00:21,  7.02s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4114420413970947
start make_pred
done makepred -0.00782012939453125
start save
4.76837158203125e-07

 99%|█████████▉| 196/198 [22:49<00:14,  7.11s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411574363708496
start make_pred
done makepred -0.0075321197509765625
start save
7.152557373046875e-07

 99%|█████████▉| 197/198 [22:56<00:07,  7.13s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411480665206909
start make_pred
done makepred -0.007496356964111328
start save
4.76837158203125e-07

100%|██████████| 198/198 [23:04<00:00,  7.18s/it]
100%|██████████| 198/198 [23:04<00:00,  6.99s/it]
/scratch_net/schusch/qimaqi/miniconda3/envs/feature_3dgs/lib/python3.9/site-packages/pytorch_lightning/utilities/migration/migration.py:208: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.
Lightning automatically upgraded your loaded checkpoint from v1.3.5 to v2.2.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint demo_e200.ckpt`
** Use norm [0.5, 0.5, 0.5], [0.5, 0.5, 0.5] as the mean and std **
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/5ZKStnWn8Zo_15/images
load /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/5ZKStnWn8Zo_15/images as image directroy for FolderLoader
LSegModule(
  (net): LSegNet(
    (clip_pretrained): CLIP(
      (visual): VisionTransformer(
        (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): Transformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (pretrained): Module(
      (model): VisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (pos_drop): Dropout(p=0.0, inplace=False)
        (patch_drop): Identity()
        (norm_pre): Identity()
        (blocks): Sequential(
          (0): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (1): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (2): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (3): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (4): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (5): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (6): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (7): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (8): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (9): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (10): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (11): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (12): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (13): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (14): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (15): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (16): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (17): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (18): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (19): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (20): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (21): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (22): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (23): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (fc_norm): Identity()
        (head_drop): Dropout(p=0.0, inplace=False)
        (head): Linear(in_features=1024, out_features=1000, bias=True)
      )
      (act_postprocess1): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
      )
      (act_postprocess2): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))
        (4): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))
      )
      (act_postprocess3): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (act_postprocess4): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
        (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
    (scratch): Module(
      (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (refinenet1): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet2): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet3): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet4): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (head1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
      (output_conv): Sequential(
        (0): Interpolate()
      )
    )
  )
)
scales [0.75, 1.0, 1.25, 1.75]
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/5ZKStnWn8Zo_15/images
outdir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/5ZKStnWn8Zo_15/rgb_feature_langseg
MultiEvalModule: base_size 520, crop_size 480

  0%|          | 0/108 [00:00<?, ?it/s]150
w, h = 320 256
scales [0.75, 1.0, 1.25, 1.75]
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/5ZKStnWn8Zo_15/images
outdir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/5ZKStnWn8Zo_15/rgb_feature_langseg
torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4051718711853027
start make_pred
done makepred -0.00784754753112793
calculate PCA based on 1st image 613e475cf67c48c895c9a786f82a318f_i0_0.jpg
PCA(n_components=3, random_state=42)
pca.explained_variance_ratio_ [0.6098694801330566, 0.2985202372074127, 0.027781575918197632]
pca.singular_values_ [55.942138671875, 39.13880157470703, 11.939855575561523]
-0.612939977645874 0.5863911509513855
start save
4.76837158203125e-07

  1%|          | 1/108 [00:07<13:24,  7.52s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4065780639648438
start make_pred
done makepred -0.007863283157348633
start save
1.430511474609375e-06

  2%|▏         | 2/108 [00:15<13:15,  7.50s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4067893028259277
start make_pred
done makepred -0.007858514785766602
start save
1.1920928955078125e-06

  3%|▎         | 3/108 [00:22<12:47,  7.31s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.408520221710205
start make_pred
done makepred -0.007420063018798828
start save
7.152557373046875e-07

  4%|▎         | 4/108 [00:28<12:22,  7.14s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.410068988800049
start make_pred
done makepred -0.007494449615478516
start save
7.152557373046875e-07

  5%|▍         | 5/108 [00:35<12:07,  7.06s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.406564235687256
start make_pred
done makepred -0.007816314697265625
start save
7.152557373046875e-07

  6%|▌         | 6/108 [00:43<12:11,  7.18s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412907838821411
start make_pred
done makepred -0.00735783576965332
start save
1.1920928955078125e-06

  6%|▋         | 7/108 [00:50<12:01,  7.14s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4110138416290283
start make_pred
done makepred -0.007863044738769531
start save
7.152557373046875e-07

  7%|▋         | 8/108 [00:57<12:00,  7.21s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4103634357452393
start make_pred
done makepred -0.007850408554077148
start save
7.152557373046875e-07

  8%|▊         | 9/108 [01:04<11:53,  7.20s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4112868309020996
start make_pred
done makepred -0.007822751998901367
start save
9.5367431640625e-07

  9%|▉         | 10/108 [01:12<11:44,  7.19s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411065101623535
start make_pred
done makepred -0.007432222366333008
start save
7.152557373046875e-07

 10%|█         | 11/108 [01:19<11:36,  7.18s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413853406906128
start make_pred
done makepred -0.00744175910949707
start save
1.1920928955078125e-06

 11%|█         | 12/108 [01:26<11:28,  7.17s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4110524654388428
start make_pred
done makepred -0.007806062698364258
start save
1.6689300537109375e-06

 12%|█▏        | 13/108 [01:33<11:19,  7.16s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413541078567505
start make_pred
done makepred -0.007823467254638672
start save
7.152557373046875e-07

 13%|█▎        | 14/108 [01:40<11:05,  7.08s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4123666286468506
start make_pred
done makepred -0.007843255996704102
start save
9.5367431640625e-07

 14%|█▍        | 15/108 [01:47<10:58,  7.08s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.546963691711426
start make_pred
done makepred -0.007253885269165039
start save
9.5367431640625e-07

 15%|█▍        | 16/108 [01:54<10:50,  7.07s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4078011512756348
start make_pred
done makepred -0.00785684585571289
start save
9.5367431640625e-07

 16%|█▌        | 17/108 [02:01<10:47,  7.12s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4088141918182373
start make_pred
done makepred -0.007854461669921875
start save
7.152557373046875e-07

 17%|█▋        | 18/108 [02:08<10:38,  7.10s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4120707511901855
start make_pred
done makepred -0.0074956417083740234
start save
9.5367431640625e-07

 18%|█▊        | 19/108 [02:15<10:34,  7.13s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4115302562713623
start make_pred
done makepred -0.007873058319091797
start save
7.152557373046875e-07

 19%|█▊        | 20/108 [02:23<10:38,  7.25s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.410658359527588
start make_pred
done makepred -0.007844924926757812
start save
9.5367431640625e-07

 19%|█▉        | 21/108 [02:30<10:21,  7.15s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4117443561553955
start make_pred
done makepred -0.0077860355377197266
start save
1.1920928955078125e-06

 20%|██        | 22/108 [02:37<10:13,  7.13s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412659168243408
start make_pred
done makepred -0.007821083068847656
start save
7.152557373046875e-07

 21%|██▏       | 23/108 [02:44<10:02,  7.09s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.40946626663208
start make_pred
done makepred -0.007544040679931641
start save
9.5367431640625e-07

 22%|██▏       | 24/108 [02:51<09:51,  7.05s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413261890411377
start make_pred
done makepred -0.007462739944458008
start save
1.1920928955078125e-06

 23%|██▎       | 25/108 [02:58<09:55,  7.18s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4119319915771484
start make_pred
done makepred -0.007847309112548828
start save
9.5367431640625e-07

 24%|██▍       | 26/108 [03:06<09:55,  7.26s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4139652252197266
start make_pred
done makepred -0.007879972457885742
start save
9.5367431640625e-07

 25%|██▌       | 27/108 [03:13<09:51,  7.31s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4128599166870117
start make_pred
done makepred -0.007512092590332031
start save
9.5367431640625e-07

 26%|██▌       | 28/108 [03:20<09:40,  7.25s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4134442806243896
start make_pred
done makepred -0.007488727569580078
start save
4.76837158203125e-07

 27%|██▋       | 29/108 [03:28<09:36,  7.29s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.415994644165039
start make_pred
done makepred -0.007879257202148438
start save
1.1920928955078125e-06

 28%|██▊       | 30/108 [03:35<09:28,  7.29s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4133381843566895
start make_pred
done makepred -0.007896184921264648
start save
1.1920928955078125e-06

 29%|██▊       | 31/108 [03:42<09:22,  7.31s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411867618560791
start make_pred
done makepred -0.007912874221801758
start save
9.5367431640625e-07

 30%|██▉       | 32/108 [03:50<09:20,  7.38s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.41332745552063
start make_pred
done makepred -0.007864236831665039
start save
7.152557373046875e-07

 31%|███       | 33/108 [03:57<09:05,  7.28s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4141180515289307
start make_pred
done makepred -0.007838249206542969
start save
9.5367431640625e-07

 31%|███▏      | 34/108 [04:04<08:57,  7.26s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4131436347961426
start make_pred
done makepred -0.007524251937866211
start save
9.5367431640625e-07

 32%|███▏      | 35/108 [04:12<08:54,  7.32s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413482666015625
start make_pred
done makepred -0.007846593856811523
start save
1.6689300537109375e-06

 33%|███▎      | 36/108 [04:19<08:44,  7.28s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412280321121216
start make_pred
done makepred -0.007856130599975586
start save
1.430511474609375e-06

 34%|███▍      | 37/108 [04:26<08:39,  7.32s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4116783142089844
start make_pred
done makepred -0.007781267166137695
start save
9.5367431640625e-07

 35%|███▌      | 38/108 [04:33<08:24,  7.20s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4113144874572754
start make_pred
done makepred -0.007879495620727539
start save
7.152557373046875e-07

 36%|███▌      | 39/108 [04:40<08:13,  7.15s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412181854248047
start make_pred
done makepred -0.007391929626464844
start save
1.1920928955078125e-06

 37%|███▋      | 40/108 [04:47<07:58,  7.03s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4076595306396484
start make_pred
done makepred -0.007851600646972656
start save
9.5367431640625e-07

 38%|███▊      | 41/108 [04:54<07:47,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4092135429382324
start make_pred
done makepred -0.007833003997802734
start save
9.5367431640625e-07

 39%|███▉      | 42/108 [05:01<07:36,  6.92s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.409238338470459
start make_pred
done makepred -0.007866382598876953
start save
4.76837158203125e-07

 40%|███▉      | 43/108 [05:08<07:29,  6.91s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4153499603271484
start make_pred
done makepred -0.007802248001098633
start save
1.1920928955078125e-06

 41%|████      | 44/108 [05:15<07:24,  6.94s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.409306526184082
start make_pred
done makepred -0.007382392883300781
start save
9.5367431640625e-07

 42%|████▏     | 45/108 [05:21<07:13,  6.88s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4128167629241943
start make_pred
done makepred -0.007873296737670898
start save
4.76837158203125e-07

 43%|████▎     | 46/108 [05:28<07:08,  6.91s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4142370223999023
start make_pred
done makepred -0.007835865020751953
start save
1.1920928955078125e-06

 44%|████▎     | 47/108 [05:35<07:05,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4129815101623535
start make_pred
done makepred -0.007839679718017578
start save
1.1920928955078125e-06

 44%|████▍     | 48/108 [05:43<07:02,  7.04s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.415435552597046
start make_pred
done makepred -0.007502079010009766
start save
1.1920928955078125e-06

 45%|████▌     | 49/108 [05:50<06:55,  7.04s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.416309118270874
start make_pred
done makepred -0.00785374641418457
start save
9.5367431640625e-07

 46%|████▋     | 50/108 [05:57<06:54,  7.14s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.414479970932007
start make_pred
done makepred -0.0078084468841552734
start save
7.152557373046875e-07

 47%|████▋     | 51/108 [06:04<06:42,  7.06s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413510322570801
start make_pred
done makepred -0.007859945297241211
start save
7.152557373046875e-07

 48%|████▊     | 52/108 [06:11<06:37,  7.09s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.414242744445801
start make_pred
done makepred -0.00787043571472168
start save
4.76837158203125e-07

 49%|████▉     | 53/108 [06:18<06:33,  7.15s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4123213291168213
start make_pred
done makepred -0.007445335388183594
start save
4.76837158203125e-07

 50%|█████     | 54/108 [06:26<06:27,  7.17s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4103853702545166
start make_pred
done makepred -0.007569074630737305
start save
7.152557373046875e-07

 51%|█████     | 55/108 [06:32<06:14,  7.07s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4061896800994873
start make_pred
done makepred -0.007831811904907227
start save
4.76837158203125e-07

 52%|█████▏    | 56/108 [06:39<06:06,  7.05s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4133286476135254
start make_pred
done makepred -0.007870674133300781
start save
7.152557373046875e-07

 53%|█████▎    | 57/108 [06:46<06:00,  7.06s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413519859313965
start make_pred
done makepred -0.007842540740966797
start save
9.5367431640625e-07

 54%|█████▎    | 58/108 [06:54<05:52,  7.05s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4137938022613525
start make_pred
done makepred -0.00785684585571289
start save
4.76837158203125e-07

 55%|█████▍    | 59/108 [07:01<05:46,  7.07s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4127256870269775
start make_pred
done makepred -0.007869243621826172
start save
1.430511474609375e-06

 56%|█████▌    | 60/108 [07:08<05:37,  7.03s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4129488468170166
start make_pred
done makepred -0.007827520370483398
start save
1.1920928955078125e-06

 56%|█████▋    | 61/108 [07:15<05:31,  7.05s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412147045135498
start make_pred
done makepred -0.007573366165161133
start save
1.6689300537109375e-06

 57%|█████▋    | 62/108 [07:22<05:23,  7.04s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4103338718414307
start make_pred
done makepred -0.007890462875366211
start save
7.152557373046875e-07

 58%|█████▊    | 63/108 [07:29<05:15,  7.02s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4127590656280518
start make_pred
done makepred -0.007062435150146484
start save
7.152557373046875e-07

 59%|█████▉    | 64/108 [07:36<05:07,  6.99s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.414064407348633
start make_pred
done makepred -0.007856607437133789
start save
7.152557373046875e-07

 60%|██████    | 65/108 [07:43<05:00,  6.99s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4133286476135254
start make_pred
done makepred -0.00782632827758789
start save
7.152557373046875e-07

 61%|██████    | 66/108 [07:49<04:50,  6.91s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4141805171966553
start make_pred
done makepred -0.007512569427490234
start save
9.5367431640625e-07

 62%|██████▏   | 67/108 [07:57<04:50,  7.08s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413686752319336
start make_pred
done makepred -0.007552385330200195
start save
7.152557373046875e-07

 63%|██████▎   | 68/108 [08:04<04:43,  7.08s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411491632461548
start make_pred
done makepred -0.00787973403930664
start save
9.5367431640625e-07

 64%|██████▍   | 69/108 [08:11<04:36,  7.09s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412973642349243
start make_pred
done makepred -0.007887840270996094
start save
7.152557373046875e-07

 65%|██████▍   | 70/108 [08:18<04:34,  7.22s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412114381790161
start make_pred
done makepred -0.007917642593383789
start save
4.76837158203125e-07

 66%|██████▌   | 71/108 [08:26<04:29,  7.27s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413710355758667
start make_pred
done makepred -0.007913351058959961
start save
1.430511474609375e-06

 67%|██████▋   | 72/108 [08:33<04:18,  7.19s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4155051708221436
start make_pred
done makepred -0.007942676544189453
start save
7.152557373046875e-07

 68%|██████▊   | 73/108 [08:40<04:11,  7.19s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.5601844787597656
start make_pred
done makepred -0.007365703582763672
start save
9.5367431640625e-07

 69%|██████▊   | 74/108 [08:47<04:06,  7.26s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412288188934326
start make_pred
done makepred -0.007275819778442383
start save
9.5367431640625e-07

 69%|██████▉   | 75/108 [08:55<03:58,  7.22s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4106225967407227
start make_pred
done makepred -0.007798194885253906
start save
1.430511474609375e-06

 70%|███████   | 76/108 [09:02<03:49,  7.16s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4126393795013428
start make_pred
done makepred -0.007777690887451172
start save
7.152557373046875e-07

 71%|███████▏  | 77/108 [09:09<03:41,  7.14s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4103262424468994
start make_pred
done makepred -0.007718086242675781
start save
7.152557373046875e-07

 72%|███████▏  | 78/108 [09:16<03:33,  7.11s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4151737689971924
start make_pred
done makepred -0.007878780364990234
start save
9.5367431640625e-07

 73%|███████▎  | 79/108 [09:23<03:26,  7.13s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4142987728118896
start make_pred
done makepred -0.007599830627441406
start save
4.76837158203125e-07

 74%|███████▍  | 80/108 [09:30<03:20,  7.15s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.414235830307007
start make_pred
done makepred -0.0075931549072265625
start save
7.152557373046875e-07

 75%|███████▌  | 81/108 [09:37<03:13,  7.16s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4126856327056885
start make_pred
done makepred -0.007919073104858398
start save
7.152557373046875e-07

 76%|███████▌  | 82/108 [09:44<03:06,  7.16s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412986993789673
start make_pred
done makepred -0.007904767990112305
start save
7.152557373046875e-07

 77%|███████▋  | 83/108 [09:52<02:59,  7.17s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4128458499908447
start make_pred
done makepred -0.007860183715820312
start save
9.5367431640625e-07

 78%|███████▊  | 84/108 [09:59<02:55,  7.30s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412238359451294
start make_pred
done makepred -0.007958173751831055
start save
1.1920928955078125e-06

 79%|███████▊  | 85/108 [10:06<02:47,  7.26s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412480354309082
start make_pred
done makepred -0.007861614227294922
start save
1.430511474609375e-06

 80%|███████▉  | 86/108 [10:13<02:37,  7.17s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.410092353820801
start make_pred
done makepred -0.007862567901611328
start save
9.5367431640625e-07

 81%|████████  | 87/108 [10:21<02:32,  7.24s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4119250774383545
start make_pred
done makepred -0.00795292854309082
start save
7.152557373046875e-07

 81%|████████▏ | 88/108 [10:28<02:26,  7.33s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.410808563232422
start make_pred
done makepred -0.00792241096496582
start save
9.5367431640625e-07

 82%|████████▏ | 89/108 [10:36<02:19,  7.37s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411334991455078
start make_pred
done makepred -0.007876396179199219
start save
1.430511474609375e-06

 83%|████████▎ | 90/108 [10:43<02:11,  7.29s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4106552600860596
start make_pred
done makepred -0.007958173751831055
start save
1.1920928955078125e-06

 84%|████████▍ | 91/108 [10:50<02:04,  7.31s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4100794792175293
start make_pred
done makepred -0.007546424865722656
start save
1.1920928955078125e-06

 85%|████████▌ | 92/108 [10:58<01:57,  7.31s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4089488983154297
start make_pred
done makepred -0.00791168212890625
start save
4.76837158203125e-07

 86%|████████▌ | 93/108 [11:05<01:48,  7.21s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.409923791885376
start make_pred
done makepred -0.007961034774780273
start save
1.1920928955078125e-06

 87%|████████▋ | 94/108 [11:12<01:41,  7.25s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4126479625701904
start make_pred
done makepred -0.008013248443603516
start save
4.76837158203125e-07

 88%|████████▊ | 95/108 [11:19<01:35,  7.34s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4116480350494385
start make_pred
done makepred -0.007592678070068359
start save
7.152557373046875e-07

 89%|████████▉ | 96/108 [11:27<01:27,  7.27s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.414349317550659
start make_pred
done makepred -0.007908344268798828
start save
9.5367431640625e-07

 90%|████████▉ | 97/108 [11:34<01:20,  7.30s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4132702350616455
start make_pred
done makepred -0.007902383804321289
start save
7.152557373046875e-07

 91%|█████████ | 98/108 [11:41<01:13,  7.31s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.414031744003296
start make_pred
done makepred -0.007948160171508789
start save
7.152557373046875e-07

 92%|█████████▏| 99/108 [11:49<01:05,  7.31s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413325309753418
start make_pred
done makepred -0.007910966873168945
start save
1.1920928955078125e-06

 93%|█████████▎| 100/108 [11:56<00:57,  7.21s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4135847091674805
start make_pred
done makepred -0.007602691650390625
start save
9.5367431640625e-07

 94%|█████████▎| 101/108 [12:03<00:50,  7.24s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4139480590820312
start make_pred
done makepred -0.007593631744384766
start save
7.152557373046875e-07

 94%|█████████▍| 102/108 [12:10<00:43,  7.23s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4134552478790283
start make_pred
done makepred -0.007554531097412109
start save
7.152557373046875e-07

 95%|█████████▌| 103/108 [12:17<00:35,  7.17s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.414024591445923
start make_pred
done makepred -0.007918357849121094
start save
7.152557373046875e-07

 96%|█████████▋| 104/108 [12:24<00:28,  7.11s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.41169810295105
start make_pred
done makepred -0.007926464080810547
start save
7.152557373046875e-07

 97%|█████████▋| 105/108 [12:32<00:21,  7.24s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4114274978637695
start make_pred
done makepred -0.007925271987915039
start save
9.5367431640625e-07

 98%|█████████▊| 106/108 [12:39<00:14,  7.19s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412519693374634
start make_pred
done makepred -0.00790715217590332
start save
4.76837158203125e-07

 99%|█████████▉| 107/108 [12:46<00:07,  7.18s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4110653400421143
start make_pred
done makepred -0.007921695709228516
start save
4.76837158203125e-07

100%|██████████| 108/108 [12:53<00:00,  7.16s/it]
100%|██████████| 108/108 [12:53<00:00,  7.16s/it]
/scratch_net/schusch/qimaqi/miniconda3/envs/feature_3dgs/lib/python3.9/site-packages/pytorch_lightning/utilities/migration/migration.py:208: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.
Lightning automatically upgraded your loaded checkpoint from v1.3.5 to v2.2.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint demo_e200.ckpt`
** Use norm [0.5, 0.5, 0.5], [0.5, 0.5, 0.5] as the mean and std **
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/pa4otMbVnkk_27/images
load /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/pa4otMbVnkk_27/images as image directroy for FolderLoader
LSegModule(
  (net): LSegNet(
    (clip_pretrained): CLIP(
      (visual): VisionTransformer(
        (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): Transformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (pretrained): Module(
      (model): VisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (pos_drop): Dropout(p=0.0, inplace=False)
        (patch_drop): Identity()
        (norm_pre): Identity()
        (blocks): Sequential(
          (0): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (1): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (2): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (3): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (4): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (5): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (6): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (7): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (8): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (9): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (10): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (11): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (12): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (13): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (14): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (15): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (16): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (17): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (18): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (19): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (20): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (21): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (22): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (23): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (fc_norm): Identity()
        (head_drop): Dropout(p=0.0, inplace=False)
        (head): Linear(in_features=1024, out_features=1000, bias=True)
      )
      (act_postprocess1): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
      )
      (act_postprocess2): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))
        (4): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))
      )
      (act_postprocess3): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (act_postprocess4): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
        (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
    (scratch): Module(
      (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (refinenet1): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet2): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet3): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet4): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (head1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
      (output_conv): Sequential(
        (0): Interpolate()
      )
    )
  )
)
scales [0.75, 1.0, 1.25, 1.75]
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/pa4otMbVnkk_27/images
outdir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/pa4otMbVnkk_27/rgb_feature_langseg
MultiEvalModule: base_size 520, crop_size 480

  0%|          | 0/54 [00:00<?, ?it/s]150
w, h = 320 256
scales [0.75, 1.0, 1.25, 1.75]
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/pa4otMbVnkk_27/images
outdir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/pa4otMbVnkk_27/rgb_feature_langseg
torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.423574686050415
start make_pred
done makepred -0.007562398910522461
calculate PCA based on 1st image 520b5be424a84062bdd6e8a6aa4a40b5_i0_0.jpg
PCA(n_components=3, random_state=42)
pca.explained_variance_ratio_ [0.6548118591308594, 0.14542752504348755, 0.07609058171510696]
pca.singular_values_ [33.191925048828125, 15.642187118530273, 11.314611434936523]
-0.39209164381027223 0.25874220132827763
start save
9.5367431640625e-07

  2%|▏         | 1/54 [00:07<06:29,  7.35s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4066951274871826
start make_pred
done makepred -0.007559776306152344
start save
1.1920928955078125e-06

  4%|▎         | 2/54 [00:14<06:17,  7.26s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4104433059692383
start make_pred
done makepred -0.007909536361694336
start save
7.152557373046875e-07

  6%|▌         | 3/54 [00:21<06:08,  7.22s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411081075668335
start make_pred
done makepred -0.007884025573730469
start save
1.1920928955078125e-06

  7%|▋         | 4/54 [00:28<06:00,  7.21s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.409755229949951
start make_pred
done makepred -0.007570505142211914
start save
1.430511474609375e-06

  9%|▉         | 5/54 [00:35<05:48,  7.10s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4070041179656982
start make_pred
done makepred -0.0075833797454833984
start save
4.76837158203125e-07

 11%|█         | 6/54 [00:42<05:40,  7.09s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.410733699798584
start make_pred
done makepred -0.00788116455078125
start save
9.5367431640625e-07

 13%|█▎        | 7/54 [00:49<05:31,  7.04s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.409770965576172
start make_pred
done makepred -0.007910966873168945
start save
1.9073486328125e-06

 15%|█▍        | 8/54 [00:56<05:20,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4292473793029785
start make_pred
done makepred -0.007897377014160156
start save
4.76837158203125e-07

 17%|█▋        | 9/54 [01:03<05:08,  6.86s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.428313970565796
start make_pred
done makepred -0.007936954498291016
start save
1.1920928955078125e-06

 19%|█▊        | 10/54 [01:10<05:07,  6.98s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412236213684082
start make_pred
done makepred -0.007898330688476562
start save
7.152557373046875e-07

 20%|██        | 11/54 [01:16<04:52,  6.81s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.426234245300293
start make_pred
done makepred -0.00791621208190918
start save
9.5367431640625e-07

 22%|██▏       | 12/54 [01:23<04:46,  6.82s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.414705276489258
start make_pred
done makepred -0.00787496566772461
start save
4.76837158203125e-07

 24%|██▍       | 13/54 [01:30<04:41,  6.87s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4130237102508545
start make_pred
done makepred -0.007595539093017578
start save
9.5367431640625e-07

 26%|██▌       | 14/54 [01:37<04:35,  6.89s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4162182807922363
start make_pred
done makepred -0.0075418949127197266
start save
4.76837158203125e-07

 28%|██▊       | 15/54 [01:44<04:29,  6.92s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4143435955047607
start make_pred
done makepred -0.007918357849121094
start save
1.1920928955078125e-06

 30%|██▉       | 16/54 [01:51<04:27,  7.03s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.414771795272827
start make_pred
done makepred -0.007902383804321289
start save
4.76837158203125e-07

 31%|███▏      | 17/54 [01:59<04:22,  7.08s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412264108657837
start make_pred
done makepred -0.0079803466796875
start save
7.152557373046875e-07

 33%|███▎      | 18/54 [02:06<04:12,  7.02s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4122257232666016
start make_pred
done makepred -0.007973670959472656
start save
4.76837158203125e-07

 35%|███▌      | 19/54 [02:13<04:07,  7.06s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.5426385402679443
start make_pred
done makepred -0.007845640182495117
start save
1.430511474609375e-06

 37%|███▋      | 20/54 [02:20<04:04,  7.18s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4119293689727783
start make_pred
done makepred -0.007950544357299805
start save
1.430511474609375e-06

 39%|███▉      | 21/54 [02:27<03:55,  7.14s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.409709930419922
start make_pred
done makepred -0.0074918270111083984
start save
7.152557373046875e-07

 41%|████      | 22/54 [02:35<03:50,  7.19s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4125516414642334
start make_pred
done makepred -0.007899999618530273
start save
4.76837158203125e-07

 43%|████▎     | 23/54 [02:42<03:40,  7.12s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413140058517456
start make_pred
done makepred -0.00790095329284668
start save
4.76837158203125e-07

 44%|████▍     | 24/54 [02:49<03:33,  7.10s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412820816040039
start make_pred
done makepred -0.007929325103759766
start save
7.152557373046875e-07

 46%|████▋     | 25/54 [02:55<03:23,  7.03s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4143993854522705
start make_pred
done makepred -0.007918834686279297
start save
9.5367431640625e-07

 48%|████▊     | 26/54 [03:02<03:15,  6.98s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.409445285797119
start make_pred
done makepred -0.007547616958618164
start save
1.1920928955078125e-06

 50%|█████     | 27/54 [03:09<03:08,  6.98s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413632392883301
start make_pred
done makepred -0.007576704025268555
start save
7.152557373046875e-07

 52%|█████▏    | 28/54 [03:16<03:01,  6.98s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.415417194366455
start make_pred
done makepred -0.007918596267700195
start save
7.152557373046875e-07

 54%|█████▎    | 29/54 [03:23<02:53,  6.93s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4120676517486572
start make_pred
done makepred -0.007860898971557617
start save
1.1920928955078125e-06

 56%|█████▌    | 30/54 [03:30<02:45,  6.88s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.416901111602783
start make_pred
done makepred -0.007857561111450195
start save
1.1920928955078125e-06

 57%|█████▋    | 31/54 [03:37<02:39,  6.92s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413935899734497
start make_pred
done makepred -0.007899761199951172
start save
9.5367431640625e-07

 59%|█████▉    | 32/54 [03:44<02:31,  6.88s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.414041519165039
start make_pred
done makepred -0.007698535919189453
start save
1.430511474609375e-06

 61%|██████    | 33/54 [03:50<02:21,  6.76s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.429802179336548
start make_pred
done makepred -0.00788736343383789
start save
9.5367431640625e-07

 63%|██████▎   | 34/54 [03:57<02:13,  6.69s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.431748390197754
start make_pred
done makepred -0.00767827033996582
start save
7.152557373046875e-07

 65%|██████▍   | 35/54 [04:03<02:05,  6.59s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.430494546890259
start make_pred
done makepred -0.007558345794677734
start save
1.1920928955078125e-06

 67%|██████▋   | 36/54 [04:09<01:56,  6.46s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4304356575012207
start make_pred
done makepred -0.007600307464599609
start save
1.1920928955078125e-06

 69%|██████▊   | 37/54 [04:15<01:46,  6.26s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.429299831390381
start make_pred
done makepred -0.007869482040405273
start save
1.6689300537109375e-06

 70%|███████   | 38/54 [04:21<01:37,  6.10s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413956880569458
start make_pred
done makepred -0.007866859436035156
start save
4.76837158203125e-07

 72%|███████▏  | 39/54 [04:28<01:35,  6.37s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.5521247386932373
start make_pred
done makepred -0.007767438888549805
start save
1.1920928955078125e-06

 74%|███████▍  | 40/54 [04:35<01:33,  6.71s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4155492782592773
start make_pred
done makepred -0.0075836181640625
start save
1.430511474609375e-06

 76%|███████▌  | 41/54 [04:42<01:28,  6.81s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.41210675239563
start make_pred
done makepred -0.007616281509399414
start save
1.1920928955078125e-06

 78%|███████▊  | 42/54 [04:49<01:22,  6.84s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4145140647888184
start make_pred
done makepred -0.007895469665527344
start save
1.1920928955078125e-06

 80%|███████▉  | 43/54 [04:56<01:16,  6.95s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.414591073989868
start make_pred
done makepred -0.007831573486328125
start save
9.5367431640625e-07

 81%|████████▏ | 44/54 [05:03<01:08,  6.89s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.428072690963745
start make_pred
done makepred -0.0078051090240478516
start save
2.86102294921875e-06

 83%|████████▎ | 45/54 [05:10<01:00,  6.76s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4319844245910645
start make_pred
done makepred -0.007818937301635742
start save
1.1920928955078125e-06

 85%|████████▌ | 46/54 [05:16<00:52,  6.54s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4313602447509766
start make_pred
done makepred -0.007842779159545898
start save
1.1920928955078125e-06

 87%|████████▋ | 47/54 [05:22<00:45,  6.51s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4322550296783447
start make_pred
done makepred -0.007903814315795898
start save
1.430511474609375e-06

 89%|████████▉ | 48/54 [05:28<00:37,  6.26s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.418065309524536
start make_pred
done makepred -0.007895708084106445
start save
9.5367431640625e-07

 91%|█████████ | 49/54 [05:34<00:30,  6.19s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4325637817382812
start make_pred
done makepred -0.007908821105957031
start save
1.1920928955078125e-06

 93%|█████████▎| 50/54 [05:39<00:24,  6.06s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4204676151275635
start make_pred
done makepred -0.00789952278137207
start save
1.1920928955078125e-06

 94%|█████████▍| 51/54 [05:45<00:17,  5.98s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.42826247215271
start make_pred
done makepred -0.007870674133300781
start save
1.1920928955078125e-06

 96%|█████████▋| 52/54 [05:52<00:12,  6.33s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.419074773788452
start make_pred
done makepred -0.007897615432739258
start save
1.1920928955078125e-06

 98%|█████████▊| 53/54 [05:59<00:06,  6.55s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.417597770690918
start make_pred
done makepred -0.007905006408691406
start save
1.430511474609375e-06

100%|██████████| 54/54 [06:06<00:00,  6.66s/it]
100%|██████████| 54/54 [06:06<00:00,  6.79s/it]
/scratch_net/schusch/qimaqi/miniconda3/envs/feature_3dgs/lib/python3.9/site-packages/pytorch_lightning/utilities/migration/migration.py:208: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.
Lightning automatically upgraded your loaded checkpoint from v1.3.5 to v2.2.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint demo_e200.ckpt`
** Use norm [0.5, 0.5, 0.5], [0.5, 0.5, 0.5] as the mean and std **
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/fzynW3qQPVF_00/images
load /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/fzynW3qQPVF_00/images as image directroy for FolderLoader
LSegModule(
  (net): LSegNet(
    (clip_pretrained): CLIP(
      (visual): VisionTransformer(
        (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): Transformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (pretrained): Module(
      (model): VisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (pos_drop): Dropout(p=0.0, inplace=False)
        (patch_drop): Identity()
        (norm_pre): Identity()
        (blocks): Sequential(
          (0): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (1): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (2): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (3): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (4): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (5): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (6): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (7): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (8): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (9): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (10): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (11): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (12): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (13): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (14): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (15): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (16): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (17): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (18): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (19): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (20): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (21): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (22): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (23): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (fc_norm): Identity()
        (head_drop): Dropout(p=0.0, inplace=False)
        (head): Linear(in_features=1024, out_features=1000, bias=True)
      )
      (act_postprocess1): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
      )
      (act_postprocess2): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))
        (4): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))
      )
      (act_postprocess3): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (act_postprocess4): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
        (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
    (scratch): Module(
      (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (refinenet1): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet2): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet3): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet4): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (head1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
      (output_conv): Sequential(
        (0): Interpolate()
      )
    )
  )
)
scales [0.75, 1.0, 1.25, 1.75]
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/fzynW3qQPVF_00/images
outdir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/fzynW3qQPVF_00/rgb_feature_langseg
MultiEvalModule: base_size 520, crop_size 480

  0%|          | 0/18 [00:00<?, ?it/s]150
w, h = 320 256
scales [0.75, 1.0, 1.25, 1.75]
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/fzynW3qQPVF_00/images
outdir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/fzynW3qQPVF_00/rgb_feature_langseg
torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4235217571258545
start make_pred
done makepred -0.007952451705932617
calculate PCA based on 1st image 0ab628f228a14756b3e4c22b539a7d29_i0_0.jpg
PCA(n_components=3, random_state=42)
pca.explained_variance_ratio_ [0.8434271812438965, 0.07682102173566818, 0.028655556961894035]
pca.singular_values_ [50.90339660644531, 15.362540245056152, 9.3826904296875]
-0.3927268624305725 0.4395917892456055
start save
9.5367431640625e-07

  6%|▌         | 1/18 [00:06<01:56,  6.87s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.408092498779297
start make_pred
done makepred -0.007939577102661133
start save
7.152557373046875e-07

 11%|█         | 2/18 [00:13<01:50,  6.89s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4057157039642334
start make_pred
done makepred -0.007539987564086914
start save
1.1920928955078125e-06

 17%|█▋        | 3/18 [00:20<01:38,  6.60s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.428147315979004
start make_pred
done makepred -0.007875442504882812
start save
4.76837158203125e-07

 22%|██▏       | 4/18 [00:26<01:29,  6.41s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4286608695983887
start make_pred
done makepred -0.007887601852416992
start save
9.5367431640625e-07

 28%|██▊       | 5/18 [00:31<01:20,  6.18s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4294540882110596
start make_pred
done makepred -0.007901430130004883
start save
4.76837158203125e-07

 33%|███▎      | 6/18 [00:37<01:11,  6.00s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.430161237716675
start make_pred
done makepred -0.00793147087097168
start save
9.5367431640625e-07

 39%|███▉      | 7/18 [00:43<01:04,  5.89s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.42905330657959
start make_pred
done makepred -0.007897377014160156
start save
9.5367431640625e-07

 44%|████▍     | 8/18 [00:48<00:58,  5.80s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4309868812561035
start make_pred
done makepred -0.0078277587890625
start save
1.1920928955078125e-06

 50%|█████     | 9/18 [00:54<00:51,  5.77s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4194259643554688
start make_pred
done makepred -0.007896661758422852
start save
1.1920928955078125e-06

 56%|█████▌    | 10/18 [01:00<00:46,  5.87s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4314825534820557
start make_pred
done makepred -0.007898330688476562
start save
9.5367431640625e-07

 61%|██████    | 11/18 [01:06<00:41,  5.88s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.43481183052063
start make_pred
done makepred -0.007902383804321289
start save
7.152557373046875e-07

 67%|██████▋   | 12/18 [01:12<00:35,  5.86s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4258742332458496
start make_pred
done makepred -0.007735013961791992
start save
9.5367431640625e-07

 72%|███████▏  | 13/18 [01:18<00:28,  5.79s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4299991130828857
start make_pred
done makepred -0.007773876190185547
start save
7.152557373046875e-07

 78%|███████▊  | 14/18 [01:23<00:23,  5.78s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4305787086486816
start make_pred
done makepred -0.007722139358520508
start save
7.152557373046875e-07

 83%|████████▎ | 15/18 [01:29<00:17,  5.77s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4277708530426025
start make_pred
done makepred -0.007908821105957031
start save
1.1920928955078125e-06

 89%|████████▉ | 16/18 [01:35<00:11,  5.77s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4292263984680176
start make_pred
done makepred -0.00789189338684082
start save
7.152557373046875e-07

 94%|█████████▍| 17/18 [01:40<00:05,  5.74s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.428332567214966
start make_pred
done makepred -0.007939577102661133
start save
1.6689300537109375e-06

100%|██████████| 18/18 [01:47<00:00,  6.02s/it]
100%|██████████| 18/18 [01:47<00:00,  5.98s/it]
/scratch_net/schusch/qimaqi/miniconda3/envs/feature_3dgs/lib/python3.9/site-packages/pytorch_lightning/utilities/migration/migration.py:208: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.
Lightning automatically upgraded your loaded checkpoint from v1.3.5 to v2.2.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint demo_e200.ckpt`
** Use norm [0.5, 0.5, 0.5], [0.5, 0.5, 0.5] as the mean and std **
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/rqfALeAoiTq_11/images
load /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/rqfALeAoiTq_11/images as image directroy for FolderLoader
LSegModule(
  (net): LSegNet(
    (clip_pretrained): CLIP(
      (visual): VisionTransformer(
        (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): Transformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (pretrained): Module(
      (model): VisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (pos_drop): Dropout(p=0.0, inplace=False)
        (patch_drop): Identity()
        (norm_pre): Identity()
        (blocks): Sequential(
          (0): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (1): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (2): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (3): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (4): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (5): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (6): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (7): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (8): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (9): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (10): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (11): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (12): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (13): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (14): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (15): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (16): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (17): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (18): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (19): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (20): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (21): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (22): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (23): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (fc_norm): Identity()
        (head_drop): Dropout(p=0.0, inplace=False)
        (head): Linear(in_features=1024, out_features=1000, bias=True)
      )
      (act_postprocess1): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
      )
      (act_postprocess2): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))
        (4): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))
      )
      (act_postprocess3): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (act_postprocess4): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
        (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
    (scratch): Module(
      (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (refinenet1): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet2): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet3): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet4): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (head1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
      (output_conv): Sequential(
        (0): Interpolate()
      )
    )
  )
)
scales [0.75, 1.0, 1.25, 1.75]
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/rqfALeAoiTq_11/images
outdir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/rqfALeAoiTq_11/rgb_feature_langseg
MultiEvalModule: base_size 520, crop_size 480

  0%|          | 0/54 [00:00<?, ?it/s]150
w, h = 320 256
scales [0.75, 1.0, 1.25, 1.75]
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/rqfALeAoiTq_11/images
outdir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/rqfALeAoiTq_11/rgb_feature_langseg
torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.425827741622925
start make_pred
done makepred -0.00793313980102539
calculate PCA based on 1st image 74a3ecae7ecd44a79ac38b59171f4301_i0_0.jpg
PCA(n_components=3, random_state=42)
pca.explained_variance_ratio_ [0.7751970887184143, 0.10585692524909973, 0.043526675552129745]
pca.singular_values_ [51.294429779052734, 18.954999923706055, 12.154633522033691]
-0.30554050803184507 0.5572335720062256
start save
4.76837158203125e-07

  2%|▏         | 1/54 [00:06<06:00,  6.80s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4097490310668945
start make_pred
done makepred -0.007912397384643555
start save
7.152557373046875e-07

  4%|▎         | 2/54 [00:13<05:48,  6.70s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4266812801361084
start make_pred
done makepred -0.007204532623291016
start save
1.430511474609375e-06

  6%|▌         | 3/54 [00:20<05:42,  6.72s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.407013416290283
start make_pred
done makepred -0.0075244903564453125
start save
1.1920928955078125e-06

  7%|▋         | 4/54 [00:27<05:39,  6.79s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4107494354248047
start make_pred
done makepred -0.007893085479736328
start save
1.1920928955078125e-06

  9%|▉         | 5/54 [00:33<05:29,  6.73s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.427157402038574
start make_pred
done makepred -0.00755000114440918
start save
7.152557373046875e-07

 11%|█         | 6/54 [00:40<05:20,  6.68s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4272186756134033
start make_pred
done makepred -0.00758814811706543
start save
4.76837158203125e-07

 13%|█▎        | 7/54 [00:47<05:16,  6.74s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4123480319976807
start make_pred
done makepred -0.007887840270996094
start save
7.152557373046875e-07

 15%|█▍        | 8/54 [00:53<05:09,  6.72s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4273431301116943
start make_pred
done makepred -0.007886171340942383
start save
1.1920928955078125e-06

 17%|█▋        | 9/54 [01:00<05:07,  6.84s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4131407737731934
start make_pred
done makepred -0.007599353790283203
start save
9.5367431640625e-07

 19%|█▊        | 10/54 [01:07<05:00,  6.82s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413600444793701
start make_pred
done makepred -0.007943391799926758
start save
7.152557373046875e-07

 20%|██        | 11/54 [01:14<04:51,  6.77s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4294164180755615
start make_pred
done makepred -0.0075070858001708984
start save
7.152557373046875e-07

 22%|██▏       | 12/54 [01:21<04:45,  6.80s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4128074645996094
start make_pred
done makepred -0.007532835006713867
start save
7.152557373046875e-07

 24%|██▍       | 13/54 [01:28<04:43,  6.90s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412853479385376
start make_pred
done makepred -0.007898092269897461
start save
1.1920928955078125e-06

 26%|██▌       | 14/54 [01:35<04:40,  7.02s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4140658378601074
start make_pred
done makepred -0.0073184967041015625
start save
9.5367431640625e-07

 28%|██▊       | 15/54 [01:42<04:37,  7.11s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4125723838806152
start make_pred
done makepred -0.007903337478637695
start save
4.76837158203125e-07

 30%|██▉       | 16/54 [01:49<04:28,  7.06s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4164397716522217
start make_pred
done makepred -0.007897138595581055
start save
4.76837158203125e-07

 31%|███▏      | 17/54 [01:57<04:26,  7.21s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4145681858062744
start make_pred
done makepred -0.007920503616333008
start save
1.1920928955078125e-06

 33%|███▎      | 18/54 [02:04<04:16,  7.11s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412327527999878
start make_pred
done makepred -0.007907867431640625
start save
2.384185791015625e-07

 35%|███▌      | 19/54 [02:11<04:05,  7.02s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.410130500793457
start make_pred
done makepred -0.007546663284301758
start save
7.152557373046875e-07

 37%|███▋      | 20/54 [02:18<03:57,  6.98s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4109816551208496
start make_pred
done makepred -0.007926464080810547
start save
4.76837158203125e-07

 39%|███▉      | 21/54 [02:25<03:51,  7.00s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4101319313049316
start make_pred
done makepred -0.007918596267700195
start save
7.152557373046875e-07

 41%|████      | 22/54 [02:32<03:44,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4115092754364014
start make_pred
done makepred -0.007615566253662109
start save
7.152557373046875e-07

 43%|████▎     | 23/54 [02:39<03:36,  6.99s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4123082160949707
start make_pred
done makepred -0.007866382598876953
start save
7.152557373046875e-07

 44%|████▍     | 24/54 [02:45<03:28,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4078922271728516
start make_pred
done makepred -0.007858991622924805
start save
7.152557373046875e-07

 46%|████▋     | 25/54 [02:52<03:21,  6.94s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4110519886016846
start make_pred
done makepred -0.007596492767333984
start save
9.5367431640625e-07

 48%|████▊     | 26/54 [02:59<03:14,  6.95s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4130632877349854
start make_pred
done makepred -0.007809638977050781
start save
2.1457672119140625e-06

 50%|█████     | 27/54 [03:07<03:09,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4122748374938965
start make_pred
done makepred -0.007858037948608398
start save
7.152557373046875e-07

 52%|█████▏    | 28/54 [03:13<03:01,  6.98s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.408552885055542
start make_pred
done makepred -0.007539272308349609
start save
4.76837158203125e-07

 54%|█████▎    | 29/54 [03:21<02:55,  7.03s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4124844074249268
start make_pred
done makepred -0.007904529571533203
start save
7.152557373046875e-07

 56%|█████▌    | 30/54 [03:27<02:48,  7.00s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4101145267486572
start make_pred
done makepred -0.007147789001464844
start save
7.152557373046875e-07

 57%|█████▋    | 31/54 [03:34<02:40,  6.98s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.414588212966919
start make_pred
done makepred -0.007900238037109375
start save
1.1920928955078125e-06

 59%|█████▉    | 32/54 [03:41<02:34,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4125397205352783
start make_pred
done makepred -0.007910728454589844
start save
1.6689300537109375e-06

 61%|██████    | 33/54 [03:48<02:26,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4102067947387695
start make_pred
done makepred -0.007605791091918945
start save
9.5367431640625e-07

 63%|██████▎   | 34/54 [03:55<02:19,  6.95s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411445379257202
start make_pred
done makepred -0.007941246032714844
start save
1.1920928955078125e-06

 65%|██████▍   | 35/54 [04:02<02:12,  6.96s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4125471115112305
start make_pred
done makepred -0.007863521575927734
start save
2.1457672119140625e-06

 67%|██████▋   | 36/54 [04:09<02:05,  6.95s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4112186431884766
start make_pred
done makepred -0.00764775276184082
start save
1.1920928955078125e-06

 69%|██████▊   | 37/54 [04:16<01:57,  6.90s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4118704795837402
start make_pred
done makepred -0.007885456085205078
start save
9.5367431640625e-07

 70%|███████   | 38/54 [04:23<01:52,  7.03s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411391258239746
start make_pred
done makepred -0.007909774780273438
start save
7.152557373046875e-07

 72%|███████▏  | 39/54 [04:30<01:45,  7.03s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.5447514057159424
start make_pred
done makepred -0.007127523422241211
start save
7.152557373046875e-07

 74%|███████▍  | 40/54 [04:38<01:39,  7.09s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.410191297531128
start make_pred
done makepred -0.00789189338684082
start save
7.152557373046875e-07

 76%|███████▌  | 41/54 [04:45<01:31,  7.05s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4062321186065674
start make_pred
done makepred -0.007605075836181641
start save
4.76837158203125e-07

 78%|███████▊  | 42/54 [04:51<01:24,  7.02s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412292003631592
start make_pred
done makepred -0.007884025573730469
start save
7.152557373046875e-07

 80%|███████▉  | 43/54 [04:59<01:17,  7.07s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413106918334961
start make_pred
done makepred -0.007894277572631836
start save
9.5367431640625e-07

 81%|████████▏ | 44/54 [05:06<01:10,  7.08s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.410292148590088
start make_pred
done makepred -0.007576465606689453
start save
7.152557373046875e-07

 83%|████████▎ | 45/54 [05:13<01:04,  7.17s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.409421682357788
start make_pred
done makepred -0.00788736343383789
start save
4.76837158203125e-07

 85%|████████▌ | 46/54 [05:20<00:57,  7.16s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4085750579833984
start make_pred
done makepred -0.007580757141113281
start save
7.152557373046875e-07

 87%|████████▋ | 47/54 [05:28<00:50,  7.20s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4098074436187744
start make_pred
done makepred -0.007892370223999023
start save
4.76837158203125e-07

 89%|████████▉ | 48/54 [05:35<00:43,  7.29s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4109809398651123
start make_pred
done makepred -0.007722139358520508
start save
4.76837158203125e-07

 91%|█████████ | 49/54 [05:42<00:35,  7.18s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4101758003234863
start make_pred
done makepred -0.0076220035552978516
start save
1.6689300537109375e-06

 93%|█████████▎| 50/54 [05:49<00:28,  7.23s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4125490188598633
start make_pred
done makepred -0.007888078689575195
start save
4.5299530029296875e-06

 94%|█████████▍| 51/54 [05:56<00:21,  7.17s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4121251106262207
start make_pred
done makepred -0.007566928863525391
start save
9.5367431640625e-07

 96%|█████████▋| 52/54 [06:04<00:14,  7.19s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411167860031128
start make_pred
done makepred -0.007892370223999023
start save
9.5367431640625e-07

 98%|█████████▊| 53/54 [06:11<00:07,  7.13s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411409378051758
start make_pred
done makepred -0.007867813110351562
start save
1.430511474609375e-06

100%|██████████| 54/54 [06:18<00:00,  7.12s/it]
100%|██████████| 54/54 [06:18<00:00,  7.00s/it]
/scratch_net/schusch/qimaqi/miniconda3/envs/feature_3dgs/lib/python3.9/site-packages/pytorch_lightning/utilities/migration/migration.py:208: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.
Lightning automatically upgraded your loaded checkpoint from v1.3.5 to v2.2.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint demo_e200.ckpt`
** Use norm [0.5, 0.5, 0.5], [0.5, 0.5, 0.5] as the mean and std **
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/ARNzJeq3xxb_07/images
load /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/ARNzJeq3xxb_07/images as image directroy for FolderLoader
LSegModule(
  (net): LSegNet(
    (clip_pretrained): CLIP(
      (visual): VisionTransformer(
        (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): Transformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (pretrained): Module(
      (model): VisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (pos_drop): Dropout(p=0.0, inplace=False)
        (patch_drop): Identity()
        (norm_pre): Identity()
        (blocks): Sequential(
          (0): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (1): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (2): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (3): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (4): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (5): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (6): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (7): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (8): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (9): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (10): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (11): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (12): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (13): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (14): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (15): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (16): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (17): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (18): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (19): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (20): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (21): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (22): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (23): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (fc_norm): Identity()
        (head_drop): Dropout(p=0.0, inplace=False)
        (head): Linear(in_features=1024, out_features=1000, bias=True)
      )
      (act_postprocess1): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
      )
      (act_postprocess2): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))
        (4): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))
      )
      (act_postprocess3): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (act_postprocess4): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
        (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
    (scratch): Module(
      (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (refinenet1): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet2): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet3): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet4): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (head1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
      (output_conv): Sequential(
        (0): Interpolate()
      )
    )
  )
)
scales [0.75, 1.0, 1.25, 1.75]
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/ARNzJeq3xxb_07/images
outdir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/ARNzJeq3xxb_07/rgb_feature_langseg
MultiEvalModule: base_size 520, crop_size 480

  0%|          | 0/18 [00:00<?, ?it/s]150
w, h = 320 256
scales [0.75, 1.0, 1.25, 1.75]
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/ARNzJeq3xxb_07/images
outdir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/ARNzJeq3xxb_07/rgb_feature_langseg
torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4070281982421875
start make_pred
done makepred -0.007757425308227539
calculate PCA based on 1st image a74656ded04545c2b00ab4328c5ff5d6_i0_0.jpg
PCA(n_components=3, random_state=42)
pca.explained_variance_ratio_ [0.7756329774856567, 0.12038575112819672, 0.051907334476709366]
pca.singular_values_ [58.47551727294922, 23.037399291992188, 15.127260208129883]
-0.43126034140586855 0.45735780596733094
start save
7.152557373046875e-07

  6%|▌         | 1/18 [00:07<02:05,  7.39s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.54321551322937
start make_pred
done makepred -0.007378101348876953
start save
4.76837158203125e-07

 11%|█         | 2/18 [00:14<01:56,  7.27s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.409546136856079
start make_pred
done makepred -0.007512331008911133
start save
7.152557373046875e-07

 17%|█▋        | 3/18 [00:21<01:48,  7.21s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4125289916992188
start make_pred
done makepred -0.007692813873291016
start save
1.1920928955078125e-06

 22%|██▏       | 4/18 [00:28<01:38,  7.07s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4115521907806396
start make_pred
done makepred -0.007444620132446289
start save
9.5367431640625e-07

 28%|██▊       | 5/18 [00:35<01:31,  7.04s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4118051528930664
start make_pred
done makepred -0.007898092269897461
start save
9.5367431640625e-07

 33%|███▎      | 6/18 [00:42<01:23,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4142305850982666
start make_pred
done makepred -0.007873058319091797
start save
7.152557373046875e-07

 39%|███▉      | 7/18 [00:49<01:16,  6.91s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413975954055786
start make_pred
done makepred -0.007604837417602539
start save
7.152557373046875e-07

 44%|████▍     | 8/18 [00:56<01:09,  6.93s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4127426147460938
start make_pred
done makepred -0.007908105850219727
start save
9.5367431640625e-07

 50%|█████     | 9/18 [01:03<01:02,  7.00s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4135444164276123
start make_pred
done makepred -0.007898330688476562
start save
4.76837158203125e-07

 56%|█████▌    | 10/18 [01:10<00:56,  7.04s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.415487766265869
start make_pred
done makepred -0.007649421691894531
start save
9.5367431640625e-07

 61%|██████    | 11/18 [01:17<00:48,  7.00s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4120283126831055
start make_pred
done makepred -0.007887601852416992
start save
9.5367431640625e-07

 67%|██████▋   | 12/18 [01:24<00:42,  7.04s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412175416946411
start make_pred
done makepred -0.007591724395751953
start save
4.76837158203125e-07

 72%|███████▏  | 13/18 [01:31<00:35,  7.06s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413825035095215
start make_pred
done makepred -0.007844686508178711
start save
9.5367431640625e-07

 78%|███████▊  | 14/18 [01:38<00:28,  7.08s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4128785133361816
start make_pred
done makepred -0.007910013198852539
start save
1.1920928955078125e-06

 83%|████████▎ | 15/18 [01:45<00:21,  7.11s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4146440029144287
start make_pred
done makepred -0.007628917694091797
start save
4.76837158203125e-07

 89%|████████▉ | 16/18 [01:52<00:14,  7.05s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.414893388748169
start make_pred
done makepred -0.007893562316894531
start save
9.5367431640625e-07

 94%|█████████▍| 17/18 [01:59<00:07,  7.05s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.415015459060669
start make_pred
done makepred -0.0077435970306396484
start save
7.152557373046875e-07

100%|██████████| 18/18 [02:06<00:00,  7.00s/it]
100%|██████████| 18/18 [02:06<00:00,  7.04s/it]
/scratch_net/schusch/qimaqi/miniconda3/envs/feature_3dgs/lib/python3.9/site-packages/pytorch_lightning/utilities/migration/migration.py:208: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.
Lightning automatically upgraded your loaded checkpoint from v1.3.5 to v2.2.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint demo_e200.ckpt`
** Use norm [0.5, 0.5, 0.5], [0.5, 0.5, 0.5] as the mean and std **
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/2t7WUuJeko7_02/images
load /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/2t7WUuJeko7_02/images as image directroy for FolderLoader
LSegModule(
  (net): LSegNet(
    (clip_pretrained): CLIP(
      (visual): VisionTransformer(
        (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): Transformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (pretrained): Module(
      (model): VisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (pos_drop): Dropout(p=0.0, inplace=False)
        (patch_drop): Identity()
        (norm_pre): Identity()
        (blocks): Sequential(
          (0): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (1): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (2): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (3): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (4): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (5): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (6): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (7): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (8): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (9): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (10): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (11): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (12): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (13): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (14): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (15): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (16): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (17): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (18): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (19): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (20): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (21): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (22): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (23): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (fc_norm): Identity()
        (head_drop): Dropout(p=0.0, inplace=False)
        (head): Linear(in_features=1024, out_features=1000, bias=True)
      )
      (act_postprocess1): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
      )
      (act_postprocess2): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))
        (4): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))
      )
      (act_postprocess3): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (act_postprocess4): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
        (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
    (scratch): Module(
      (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (refinenet1): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet2): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet3): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet4): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (head1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
      (output_conv): Sequential(
        (0): Interpolate()
      )
    )
  )
)
scales [0.75, 1.0, 1.25, 1.75]
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/2t7WUuJeko7_02/images
outdir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/2t7WUuJeko7_02/rgb_feature_langseg
MultiEvalModule: base_size 520, crop_size 480

  0%|          | 0/90 [00:00<?, ?it/s]150
w, h = 320 256
scales [0.75, 1.0, 1.25, 1.75]
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/2t7WUuJeko7_02/images
outdir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/2t7WUuJeko7_02/rgb_feature_langseg
torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.407578229904175
start make_pred
done makepred -0.007916688919067383
calculate PCA based on 1st image 1a0d730696cc4057b0037a75c8ef6b59_i0_0.jpg
PCA(n_components=3, random_state=42)
pca.explained_variance_ratio_ [0.47541797161102295, 0.33994919061660767, 0.14760299026966095]
pca.singular_values_ [57.73533630371094, 48.821475982666016, 32.1700325012207]
-0.6027931213378906 0.6101302981376648
start save
9.5367431640625e-07

  1%|          | 1/90 [00:07<10:49,  7.30s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.407947063446045
start make_pred
done makepred -0.00796651840209961
start save
7.152557373046875e-07

  2%|▏         | 2/90 [00:14<10:29,  7.15s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.408568859100342
start make_pred
done makepred -0.007596731185913086
start save
9.5367431640625e-07

  3%|▎         | 3/90 [00:21<10:02,  6.93s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4262804985046387
start make_pred
done makepred -0.00793313980102539
start save
1.430511474609375e-06

  4%|▍         | 4/90 [00:27<09:50,  6.87s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.426360845565796
start make_pred
done makepred -0.007901668548583984
start save
7.152557373046875e-07

  6%|▌         | 5/90 [00:34<09:54,  6.99s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.408844470977783
start make_pred
done makepred -0.007569551467895508
start save
7.152557373046875e-07

  7%|▋         | 6/90 [00:42<09:48,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4103174209594727
start make_pred
done makepred -0.007926464080810547
start save
7.152557373046875e-07

  8%|▊         | 7/90 [00:49<09:48,  7.09s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413731575012207
start make_pred
done makepred -0.0076067447662353516
start save
9.5367431640625e-07

  9%|▉         | 8/90 [00:56<09:49,  7.19s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4110777378082275
start make_pred
done makepred -0.007874250411987305
start save
4.76837158203125e-07

 10%|█         | 9/90 [01:03<09:36,  7.11s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413135528564453
start make_pred
done makepred -0.00787353515625
start save
4.76837158203125e-07

 11%|█         | 10/90 [01:10<09:33,  7.17s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4130795001983643
start make_pred
done makepred -0.007536411285400391
start save
7.152557373046875e-07

 12%|█▏        | 11/90 [01:18<09:30,  7.22s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4126996994018555
start make_pred
done makepred -0.007906198501586914
start save
7.152557373046875e-07

 13%|█▎        | 12/90 [01:25<09:22,  7.21s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411499261856079
start make_pred
done makepred -0.007600307464599609
start save
4.76837158203125e-07

 14%|█▍        | 13/90 [01:32<09:15,  7.21s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.409893751144409
start make_pred
done makepred -0.007892608642578125
start save
7.152557373046875e-07

 16%|█▌        | 14/90 [01:40<09:17,  7.33s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4110257625579834
start make_pred
done makepred -0.007642030715942383
start save
1.1920928955078125e-06

 17%|█▋        | 15/90 [01:47<09:09,  7.32s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4134981632232666
start make_pred
done makepred -0.007867813110351562
start save
9.5367431640625e-07

 18%|█▊        | 16/90 [01:54<08:50,  7.17s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4138009548187256
start make_pred
done makepred -0.007921218872070312
start save
1.1920928955078125e-06

 19%|█▉        | 17/90 [02:01<08:40,  7.13s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4133386611938477
start make_pred
done makepred -0.00759434700012207
start save
1.430511474609375e-06

 20%|██        | 18/90 [02:08<08:29,  7.08s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.549945831298828
start make_pred
done makepred -0.007833242416381836
start save
4.76837158203125e-07

 21%|██        | 19/90 [02:15<08:26,  7.13s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4117085933685303
start make_pred
done makepred -0.007393598556518555
start save
1.1920928955078125e-06

 22%|██▏       | 20/90 [02:22<08:19,  7.13s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411973237991333
start make_pred
done makepred -0.007923126220703125
start save
7.152557373046875e-07

 23%|██▎       | 21/90 [02:29<08:02,  6.99s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4263479709625244
start make_pred
done makepred -0.007892131805419922
start save
4.76837158203125e-07

 24%|██▍       | 22/90 [02:36<07:56,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4114229679107666
start make_pred
done makepred -0.0072629451751708984
start save
7.152557373046875e-07

 26%|██▌       | 23/90 [02:43<07:52,  7.05s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.410712242126465
start make_pred
done makepred -0.007920265197753906
start save
4.76837158203125e-07

 27%|██▋       | 24/90 [02:50<07:47,  7.09s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4131014347076416
start make_pred
done makepred -0.00788116455078125
start save
7.152557373046875e-07

 28%|██▊       | 25/90 [02:57<07:39,  7.07s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4149394035339355
start make_pred
done makepred -0.007611989974975586
start save
4.76837158203125e-07

 29%|██▉       | 26/90 [03:04<07:31,  7.05s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4115867614746094
start make_pred
done makepred -0.00789499282836914
start save
4.76837158203125e-07

 30%|███       | 27/90 [03:12<07:28,  7.12s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4117443561553955
start make_pred
done makepred -0.0075719356536865234
start save
7.152557373046875e-07

 31%|███       | 28/90 [03:19<07:21,  7.12s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4131574630737305
start make_pred
done makepred -0.0077860355377197266
start save
4.76837158203125e-07

 32%|███▏      | 29/90 [03:26<07:13,  7.10s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.410395622253418
start make_pred
done makepred -0.007874727249145508
start save
4.76837158203125e-07

 33%|███▎      | 30/90 [03:33<07:02,  7.05s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4138503074645996
start make_pred
done makepred -0.007600069046020508
start save
7.152557373046875e-07

 34%|███▍      | 31/90 [03:40<06:53,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4134135246276855
start make_pred
done makepred -0.0072803497314453125
start save
1.1920928955078125e-06

 36%|███▌      | 32/90 [03:47<06:47,  7.03s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4130637645721436
start make_pred
done makepred -0.007879257202148438
start save
1.9073486328125e-06

 37%|███▋      | 33/90 [03:54<06:43,  7.08s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4138107299804688
start make_pred
done makepred -0.0074024200439453125
start save
9.5367431640625e-07

 38%|███▊      | 34/90 [04:01<06:34,  7.05s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412851095199585
start make_pred
done makepred -0.007913589477539062
start save
2.384185791015625e-06

 39%|███▉      | 35/90 [04:08<06:25,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4123587608337402
start make_pred
done makepred -0.007521152496337891
start save
9.5367431640625e-07

 40%|████      | 36/90 [04:15<06:23,  7.10s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.410951614379883
start make_pred
done makepred -0.007912635803222656
start save
7.152557373046875e-07

 41%|████      | 37/90 [04:22<06:11,  7.02s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4095489978790283
start make_pred
done makepred -0.007871866226196289
start save
9.5367431640625e-07

 42%|████▏     | 38/90 [04:29<06:01,  6.96s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.41165828704834
start make_pred
done makepred -0.007620334625244141
start save
7.152557373046875e-07

 43%|████▎     | 39/90 [04:36<05:56,  6.98s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.411487579345703
start make_pred
done makepred -0.007932901382446289
start save
4.76837158203125e-07

 44%|████▍     | 40/90 [04:42<05:41,  6.84s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4281492233276367
start make_pred
done makepred -0.007892131805419922
start save
7.152557373046875e-07

 46%|████▌     | 41/90 [04:49<05:36,  6.87s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.41098952293396
start make_pred
done makepred -0.007613658905029297
start save
2.384185791015625e-07

 47%|████▋     | 42/90 [04:56<05:33,  6.94s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4129974842071533
start make_pred
done makepred -0.007888078689575195
start save
7.152557373046875e-07

 48%|████▊     | 43/90 [05:03<05:27,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.410886764526367
start make_pred
done makepred -0.007925987243652344
start save
1.1920928955078125e-06

 49%|████▉     | 44/90 [05:11<05:23,  7.03s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413738489151001
start make_pred
done makepred -0.007570505142211914
start save
1.1920928955078125e-06

 50%|█████     | 45/90 [05:18<05:17,  7.05s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.414677619934082
start make_pred
done makepred -0.007825374603271484
start save
2.384185791015625e-06

 51%|█████     | 46/90 [05:25<05:10,  7.06s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4146857261657715
start make_pred
done makepred -0.0074427127838134766
start save
1.430511474609375e-06

 52%|█████▏    | 47/90 [05:32<05:02,  7.04s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4163949489593506
start make_pred
done makepred -0.007731199264526367
start save
1.430511474609375e-06

 53%|█████▎    | 48/90 [05:39<04:53,  7.00s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4154767990112305
start make_pred
done makepred -0.007785797119140625
start save
1.1920928955078125e-06

 54%|█████▍    | 49/90 [05:46<04:47,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4218297004699707
start make_pred
done makepred -0.0067517757415771484
start save
1.430511474609375e-06

 56%|█████▌    | 50/90 [05:53<04:41,  7.03s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4192748069763184
start make_pred
done makepred -0.007687807083129883
start save
1.430511474609375e-06

 57%|█████▋    | 51/90 [06:00<04:34,  7.05s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4225027561187744
start make_pred
done makepred -0.007772684097290039
start save
1.430511474609375e-06

 58%|█████▊    | 52/90 [06:07<04:25,  6.99s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4227447509765625
start make_pred
done makepred -0.007476329803466797
start save
9.5367431640625e-07

 59%|█████▉    | 53/90 [06:14<04:19,  7.03s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.422549247741699
start make_pred
done makepred -0.007650852203369141
start save
1.9073486328125e-06

 60%|██████    | 54/90 [06:21<04:13,  7.06s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4200901985168457
start make_pred
done makepred -0.007609844207763672
start save
9.5367431640625e-07

 61%|██████    | 55/90 [06:28<04:05,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4149930477142334
start make_pred
done makepred -0.0074656009674072266
start save
9.5367431640625e-07

 62%|██████▏   | 56/90 [06:35<03:58,  7.00s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.415458917617798
start make_pred
done makepred -0.007803440093994141
start save
9.5367431640625e-07

 63%|██████▎   | 57/90 [06:42<03:53,  7.07s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4237890243530273
start make_pred
done makepred -0.007482051849365234
start save
1.430511474609375e-06

 64%|██████▍   | 58/90 [06:49<03:43,  6.99s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4160399436950684
start make_pred
done makepred -0.007708311080932617
start save
1.430511474609375e-06

 66%|██████▌   | 59/90 [06:56<03:36,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.414668321609497
start make_pred
done makepred -0.007372140884399414
start save
9.5367431640625e-07

 67%|██████▋   | 60/90 [07:02<03:26,  6.88s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4304769039154053
start make_pred
done makepred -0.007459402084350586
start save
1.1920928955078125e-06

 68%|██████▊   | 61/90 [07:09<03:19,  6.89s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.5808639526367188
start make_pred
done makepred -0.007762432098388672
start save
7.152557373046875e-07

 69%|██████▉   | 62/90 [07:16<03:12,  6.87s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4144139289855957
start make_pred
done makepred -0.007756471633911133
start save
7.152557373046875e-07

 70%|███████   | 63/90 [07:23<03:07,  6.94s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4261465072631836
start make_pred
done makepred -0.007513761520385742
start save
1.430511474609375e-06

 71%|███████   | 64/90 [07:30<03:00,  6.94s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.425848960876465
start make_pred
done makepred -0.007758140563964844
start save
1.6689300537109375e-06

 72%|███████▏  | 65/90 [07:37<02:55,  7.00s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.41473388671875
start make_pred
done makepred -0.007822513580322266
start save
1.1920928955078125e-06

 73%|███████▎  | 66/90 [07:44<02:47,  6.96s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.416095018386841
start make_pred
done makepred -0.007445573806762695
start save
9.5367431640625e-07

 74%|███████▍  | 67/90 [07:51<02:39,  6.95s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.424070358276367
start make_pred
done makepred -0.007718324661254883
start save
9.5367431640625e-07

 76%|███████▌  | 68/90 [07:58<02:34,  7.02s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4160540103912354
start make_pred
done makepred -0.007511615753173828
start save
1.430511474609375e-06

 77%|███████▋  | 69/90 [08:05<02:25,  6.95s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.418006420135498
start make_pred
done makepred -0.007446765899658203
start save
9.5367431640625e-07

 78%|███████▊  | 70/90 [08:12<02:20,  7.02s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.424426794052124
start make_pred
done makepred -0.007603168487548828
start save
2.384185791015625e-06

 79%|███████▉  | 71/90 [08:19<02:11,  6.95s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4106106758117676
start make_pred
done makepred -0.0075070858001708984
start save
9.5367431640625e-07

 80%|████████  | 72/90 [08:26<02:04,  6.94s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4149529933929443
start make_pred
done makepred -0.00777745246887207
start save
1.1920928955078125e-06

 81%|████████  | 73/90 [08:33<01:56,  6.87s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4375860691070557
start make_pred
done makepred -0.007427215576171875
start save
1.9073486328125e-06

 82%|████████▏ | 74/90 [08:40<01:49,  6.87s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.417156219482422
start make_pred
done makepred -0.007447957992553711
start save
1.1920928955078125e-06

 83%|████████▎ | 75/90 [08:47<01:43,  6.92s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4144880771636963
start make_pred
done makepred -0.007745981216430664
start save
7.152557373046875e-07

 84%|████████▍ | 76/90 [08:54<01:37,  6.94s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.417895555496216
start make_pred
done makepred -0.007668972015380859
start save
1.9073486328125e-06

 86%|████████▌ | 77/90 [09:01<01:30,  6.96s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4231514930725098
start make_pred
done makepred -0.007323503494262695
start save
1.1920928955078125e-06

 87%|████████▋ | 78/90 [09:08<01:23,  6.96s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4135093688964844
start make_pred
done makepred -0.0072650909423828125
start save
9.5367431640625e-07

 88%|████████▊ | 79/90 [09:14<01:16,  6.92s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412990093231201
start make_pred
done makepred -0.007790088653564453
start save
1.9073486328125e-06

 89%|████████▉ | 80/90 [09:22<01:09,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.425431728363037
start make_pred
done makepred -0.007416486740112305
start save
1.1920928955078125e-06

 90%|█████████ | 81/90 [09:29<01:03,  7.02s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.56040358543396
start make_pred
done makepred -0.00777888298034668
start save
7.152557373046875e-07

 91%|█████████ | 82/90 [09:36<00:56,  7.12s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4133903980255127
start make_pred
done makepred -0.007475137710571289
start save
9.5367431640625e-07

 92%|█████████▏| 83/90 [09:43<00:49,  7.05s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.421116352081299
start make_pred
done makepred -0.00765228271484375
start save
1.6689300537109375e-06

 93%|█████████▎| 84/90 [09:50<00:42,  7.04s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4155657291412354
start make_pred
done makepred -0.007815122604370117
start save
1.1920928955078125e-06

 94%|█████████▍| 85/90 [09:57<00:34,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4112401008605957
start make_pred
done makepred -0.00743865966796875
start save
1.1920928955078125e-06

 96%|█████████▌| 86/90 [10:04<00:28,  7.05s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.423330545425415
start make_pred
done makepred -0.007371425628662109
start save
9.5367431640625e-07

 97%|█████████▋| 87/90 [10:11<00:21,  7.12s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4161946773529053
start make_pred
done makepred -0.0074901580810546875
start save
1.430511474609375e-06

 98%|█████████▊| 88/90 [10:18<00:14,  7.15s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413121461868286
start make_pred
done makepred -0.007826089859008789
start save
1.1920928955078125e-06

 99%|█████████▉| 89/90 [10:26<00:07,  7.14s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.423044204711914
start make_pred
done makepred -0.007593870162963867
start save
2.384185791015625e-06

100%|██████████| 90/90 [10:33<00:00,  7.33s/it]
100%|██████████| 90/90 [10:33<00:00,  7.04s/it]
/scratch_net/schusch/qimaqi/miniconda3/envs/feature_3dgs/lib/python3.9/site-packages/pytorch_lightning/utilities/migration/migration.py:208: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.
Lightning automatically upgraded your loaded checkpoint from v1.3.5 to v2.2.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint demo_e200.ckpt`
** Use norm [0.5, 0.5, 0.5], [0.5, 0.5, 0.5] as the mean and std **
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/wc2JMjhGNzB_11/images
load /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/wc2JMjhGNzB_11/images as image directroy for FolderLoader
LSegModule(
  (net): LSegNet(
    (clip_pretrained): CLIP(
      (visual): VisionTransformer(
        (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): Transformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (pretrained): Module(
      (model): VisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (pos_drop): Dropout(p=0.0, inplace=False)
        (patch_drop): Identity()
        (norm_pre): Identity()
        (blocks): Sequential(
          (0): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (1): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (2): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (3): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (4): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (5): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (6): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (7): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (8): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (9): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (10): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (11): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (12): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (13): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (14): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (15): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (16): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (17): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (18): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (19): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (20): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (21): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (22): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (23): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (fc_norm): Identity()
        (head_drop): Dropout(p=0.0, inplace=False)
        (head): Linear(in_features=1024, out_features=1000, bias=True)
      )
      (act_postprocess1): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
      )
      (act_postprocess2): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))
        (4): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))
      )
      (act_postprocess3): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (act_postprocess4): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
        (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
    (scratch): Module(
      (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (refinenet1): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet2): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet3): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet4): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (head1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
      (output_conv): Sequential(
        (0): Interpolate()
      )
    )
  )
)
scales [0.75, 1.0, 1.25, 1.75]
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/wc2JMjhGNzB_11/images
outdir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/wc2JMjhGNzB_11/rgb_feature_langseg
MultiEvalModule: base_size 520, crop_size 480

  0%|          | 0/18 [00:00<?, ?it/s]150
w, h = 320 256
scales [0.75, 1.0, 1.25, 1.75]
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/wc2JMjhGNzB_11/images
outdir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/wc2JMjhGNzB_11/rgb_feature_langseg
torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4088799953460693
start make_pred
done makepred -0.0077893733978271484
calculate PCA based on 1st image 9d8a625f48194382a0bfb748b1126069_i0_0.jpg
PCA(n_components=3, random_state=42)
pca.explained_variance_ratio_ [0.5905681848526001, 0.2534204423427582, 0.08671378344297409]
pca.singular_values_ [52.602779388427734, 34.458351135253906, 20.156620025634766]
-0.37267547845840454 0.4983114838600159
start save
1.1920928955078125e-06

  6%|▌         | 1/18 [00:07<02:00,  7.07s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.420020818710327
start make_pred
done makepred -0.007622957229614258
start save
9.5367431640625e-07

 11%|█         | 2/18 [00:14<01:55,  7.22s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.410876512527466
start make_pred
done makepred -0.007498025894165039
start save
7.152557373046875e-07

 17%|█▋        | 3/18 [00:21<01:45,  7.04s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4070088863372803
start make_pred
done makepred -0.007798671722412109
start save
9.5367431640625e-07

 22%|██▏       | 4/18 [00:28<01:37,  6.95s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4151220321655273
start make_pred
done makepred -0.007573843002319336
start save
9.5367431640625e-07

 28%|██▊       | 5/18 [00:34<01:29,  6.91s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4206581115722656
start make_pred
done makepred -0.007401704788208008
start save
1.430511474609375e-06

 33%|███▎      | 6/18 [00:41<01:22,  6.88s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.5511322021484375
start make_pred
done makepred -0.0072367191314697266
start save
9.5367431640625e-07

 39%|███▉      | 7/18 [00:48<01:16,  6.96s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.42130184173584
start make_pred
done makepred -0.00737309455871582
start save
9.5367431640625e-07

 44%|████▍     | 8/18 [00:55<01:09,  6.99s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4113848209381104
start make_pred
done makepred -0.0078067779541015625
start save
1.1920928955078125e-06

 50%|█████     | 9/18 [01:02<01:02,  6.98s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.414229393005371
start make_pred
done makepred -0.0076906681060791016
start save
1.1920928955078125e-06

 56%|█████▌    | 10/18 [01:09<00:55,  6.94s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4218997955322266
start make_pred
done makepred -0.00740814208984375
start save
1.1920928955078125e-06

 61%|██████    | 11/18 [01:16<00:48,  6.92s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.40883731842041
start make_pred
done makepred -0.007820606231689453
start save
7.152557373046875e-07

 67%|██████▋   | 12/18 [01:23<00:41,  6.91s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4252662658691406
start make_pred
done makepred -0.007577419281005859
start save
1.1920928955078125e-06

 72%|███████▏  | 13/18 [01:30<00:35,  7.00s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4173552989959717
start make_pred
done makepred -0.007283926010131836
start save
4.291534423828125e-06

 78%|███████▊  | 14/18 [01:42<00:34,  8.52s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.418259859085083
start make_pred
done makepred -0.007411479949951172
start save
1.1920928955078125e-06

 83%|████████▎ | 15/18 [01:49<00:24,  8.05s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4088690280914307
start make_pred
done makepred -0.007775545120239258
start save
1.1920928955078125e-06

 89%|████████▉ | 16/18 [01:56<00:15,  7.74s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.419220447540283
start make_pred
done makepred -0.006861209869384766
start save
9.5367431640625e-07

 94%|█████████▍| 17/18 [02:03<00:07,  7.52s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4192302227020264
start make_pred
done makepred -0.007749319076538086
start save
9.5367431640625e-07

100%|██████████| 18/18 [02:10<00:00,  7.45s/it]
100%|██████████| 18/18 [02:10<00:00,  7.27s/it]
/scratch_net/schusch/qimaqi/miniconda3/envs/feature_3dgs/lib/python3.9/site-packages/pytorch_lightning/utilities/migration/migration.py:208: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.
Lightning automatically upgraded your loaded checkpoint from v1.3.5 to v2.2.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint demo_e200.ckpt`
** Use norm [0.5, 0.5, 0.5], [0.5, 0.5, 0.5] as the mean and std **
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/q9vSo1VnCiC_05/images
load /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/q9vSo1VnCiC_05/images as image directroy for FolderLoader
LSegModule(
  (net): LSegNet(
    (clip_pretrained): CLIP(
      (visual): VisionTransformer(
        (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): Transformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (pretrained): Module(
      (model): VisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (pos_drop): Dropout(p=0.0, inplace=False)
        (patch_drop): Identity()
        (norm_pre): Identity()
        (blocks): Sequential(
          (0): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (1): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (2): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (3): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (4): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (5): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (6): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (7): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (8): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (9): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (10): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (11): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (12): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (13): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (14): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (15): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (16): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (17): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (18): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (19): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (20): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (21): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (22): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (23): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (fc_norm): Identity()
        (head_drop): Dropout(p=0.0, inplace=False)
        (head): Linear(in_features=1024, out_features=1000, bias=True)
      )
      (act_postprocess1): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))
      )
      (act_postprocess2): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))
        (4): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))
      )
      (act_postprocess3): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (act_postprocess4): Sequential(
        (0): ProjectReadout(
          (project): Sequential(
            (0): Linear(in_features=2048, out_features=1024, bias=True)
            (1): GELU(approximate='none')
          )
        )
        (1): Transpose()
        (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))
        (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
        (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
    (scratch): Module(
      (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (refinenet1): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet2): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet3): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (refinenet4): FeatureFusionBlock_custom(
        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (resConfUnit1): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (resConfUnit2): ResidualConvUnit_custom(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): ReLU()
          (skip_add): FloatFunctional(
            (activation_post_process): Identity()
          )
        )
        (skip_add): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (head1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
      (output_conv): Sequential(
        (0): Interpolate()
      )
    )
  )
)
scales [0.75, 1.0, 1.25, 1.75]
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/q9vSo1VnCiC_05/images
outdir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/q9vSo1VnCiC_05/rgb_feature_langseg
MultiEvalModule: base_size 520, crop_size 480

  0%|          | 0/108 [00:00<?, ?it/s]150
w, h = 320 256
scales [0.75, 1.0, 1.25, 1.75]
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/q9vSo1VnCiC_05/images
outdir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/q9vSo1VnCiC_05/rgb_feature_langseg
torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4182233810424805
start make_pred
done makepred -0.00756382942199707
calculate PCA based on 1st image 64b65ee9eb5540398f325dea2113551b_i0_0.jpg
PCA(n_components=3, random_state=42)
pca.explained_variance_ratio_ [0.8472545146942139, 0.06981972604990005, 0.03935004398226738]
pca.singular_values_ [35.790367126464844, 10.274206161499023, 7.713150978088379]
-0.15748923420906066 0.4473382115364079
start save
9.5367431640625e-07

  1%|          | 1/108 [00:07<13:04,  7.33s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4103939533233643
start make_pred
done makepred -0.007775783538818359
start save
9.5367431640625e-07

  2%|▏         | 2/108 [00:14<12:37,  7.15s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.417548418045044
start make_pred
done makepred -0.007650136947631836
start save
1.1920928955078125e-06

  3%|▎         | 3/108 [00:21<12:23,  7.08s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4136905670166016
start make_pred
done makepred -0.007515668869018555
start save
1.1920928955078125e-06

  4%|▎         | 4/108 [00:28<12:19,  7.11s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.423405885696411
start make_pred
done makepred -0.0076694488525390625
start save
1.430511474609375e-06

  5%|▍         | 5/108 [00:35<12:14,  7.13s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4321324825286865
start make_pred
done makepred -0.007732868194580078
start save
9.5367431640625e-07

  6%|▌         | 6/108 [00:42<11:58,  7.05s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4272148609161377
start make_pred
done makepred -0.007585287094116211
start save
9.5367431640625e-07

  6%|▋         | 7/108 [00:49<11:54,  7.07s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4294703006744385
start make_pred
done makepred -0.00761866569519043
start save
1.430511474609375e-06

  7%|▋         | 8/108 [00:56<11:48,  7.08s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4253153800964355
start make_pred
done makepred -0.007440805435180664
start save
1.1920928955078125e-06

  8%|▊         | 9/108 [01:03<11:31,  6.99s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4493792057037354
start make_pred
done makepred -0.007632255554199219
start save
1.1920928955078125e-06

  9%|▉         | 10/108 [01:10<11:31,  7.05s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4245963096618652
start make_pred
done makepred -0.007818222045898438
start save
1.1920928955078125e-06

 10%|█         | 11/108 [01:17<11:23,  7.04s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.428105115890503
start make_pred
done makepred -0.007695913314819336
start save
1.430511474609375e-06

 11%|█         | 12/108 [01:24<11:18,  7.06s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4273681640625
start make_pred
done makepred -0.007772922515869141
start save
1.1920928955078125e-06

 12%|█▏        | 13/108 [01:32<11:13,  7.08s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4352965354919434
start make_pred
done makepred -0.0076563358306884766
start save
9.5367431640625e-07

 13%|█▎        | 14/108 [01:39<11:10,  7.13s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4238719940185547
start make_pred
done makepred -0.007782697677612305
start save
1.1920928955078125e-06

 14%|█▍        | 15/108 [01:46<10:53,  7.03s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4515464305877686
start make_pred
done makepred -0.007516145706176758
start save
1.1920928955078125e-06

 15%|█▍        | 16/108 [01:52<10:43,  6.99s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4238646030426025
start make_pred
done makepred -0.007314920425415039
start save
9.5367431640625e-07

 16%|█▌        | 17/108 [01:59<10:35,  6.99s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.427868366241455
start make_pred
done makepred -0.0076291561126708984
start save
1.430511474609375e-06

 17%|█▋        | 18/108 [02:06<10:29,  7.00s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.445117950439453
start make_pred
done makepred -0.0073893070220947266
start save
1.6689300537109375e-06

 18%|█▊        | 19/108 [02:14<10:29,  7.07s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4313318729400635
start make_pred
done makepred -0.007668256759643555
start save
9.5367431640625e-07

 19%|█▊        | 20/108 [02:20<10:12,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4404144287109375
start make_pred
done makepred -0.007973194122314453
start save
9.5367431640625e-07

 19%|█▉        | 21/108 [02:27<10:06,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4331252574920654
start make_pred
done makepred -0.0076024532318115234
start save
7.152557373046875e-07

 20%|██        | 22/108 [02:34<10:02,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4290802478790283
start make_pred
done makepred -0.00757288932800293
start save
9.5367431640625e-07

 21%|██▏       | 23/108 [02:42<09:55,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.426567792892456
start make_pred
done makepred -0.006781578063964844
start save
7.152557373046875e-07

 22%|██▏       | 24/108 [02:49<09:58,  7.12s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4263687133789062
start make_pred
done makepred -0.0074841976165771484
start save
1.1920928955078125e-06

 23%|██▎       | 25/108 [02:56<09:53,  7.15s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4262855052948
start make_pred
done makepred -0.007294654846191406
start save
9.5367431640625e-07

 24%|██▍       | 26/108 [03:03<09:40,  7.08s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.427518129348755
start make_pred
done makepred -0.007581949234008789
start save
1.1920928955078125e-06

 25%|██▌       | 27/108 [03:10<09:34,  7.10s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4211575984954834
start make_pred
done makepred -0.007398366928100586
start save
1.1920928955078125e-06

 26%|██▌       | 28/108 [03:18<09:36,  7.21s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.426356554031372
start make_pred
done makepred -0.007650852203369141
start save
7.152557373046875e-07

 27%|██▋       | 29/108 [03:25<09:24,  7.14s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4282782077789307
start make_pred
done makepred -0.007646322250366211
start save
1.430511474609375e-06

 28%|██▊       | 30/108 [03:32<09:13,  7.09s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4246044158935547
start make_pred
done makepred -0.007727146148681641
start save
9.5367431640625e-07

 29%|██▊       | 31/108 [03:39<09:16,  7.23s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.42618465423584
start make_pred
done makepred -0.00768280029296875
start save
7.152557373046875e-07

 30%|██▉       | 32/108 [03:46<09:04,  7.16s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.428828477859497
start make_pred
done makepred -0.00767064094543457
start save
9.5367431640625e-07

 31%|███       | 33/108 [03:53<08:50,  7.07s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4296951293945312
start make_pred
done makepred -0.008178234100341797
start save
9.5367431640625e-07

 31%|███▏      | 34/108 [04:00<08:39,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4282588958740234
start make_pred
done makepred -0.007672309875488281
start save
1.1920928955078125e-06

 32%|███▏      | 35/108 [04:07<08:34,  7.05s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4242725372314453
start make_pred
done makepred -0.007598161697387695
start save
1.1920928955078125e-06

 33%|███▎      | 36/108 [04:14<08:30,  7.09s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4224748611450195
start make_pred
done makepred -0.007550954818725586
start save
1.1920928955078125e-06

 34%|███▍      | 37/108 [04:21<08:20,  7.05s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4293832778930664
start make_pred
done makepred -0.007642269134521484
start save
9.5367431640625e-07

 35%|███▌      | 38/108 [04:28<08:12,  7.03s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.421325206756592
start make_pred
done makepred -0.007622480392456055
start save
1.1920928955078125e-06

 36%|███▌      | 39/108 [04:35<08:08,  7.08s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4224348068237305
start make_pred
done makepred -0.007601261138916016
start save
1.1920928955078125e-06

 37%|███▋      | 40/108 [04:43<08:02,  7.10s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.420456886291504
start make_pred
done makepred -0.007593631744384766
start save
7.152557373046875e-07

 38%|███▊      | 41/108 [04:49<07:52,  7.06s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4197285175323486
start make_pred
done makepred -0.007752656936645508
start save
1.1920928955078125e-06

 39%|███▉      | 42/108 [04:57<07:46,  7.07s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.416560649871826
start make_pred
done makepred -0.007903814315795898
start save
4.76837158203125e-07

 40%|███▉      | 43/108 [05:04<07:37,  7.04s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.414421558380127
start make_pred
done makepred -0.00792241096496582
start save
4.76837158203125e-07

 41%|████      | 44/108 [05:10<07:26,  6.98s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4143764972686768
start make_pred
done makepred -0.007648944854736328
start save
7.152557373046875e-07

 42%|████▏     | 45/108 [05:18<07:22,  7.03s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.413900375366211
start make_pred
done makepred -0.007645606994628906
start save
7.152557373046875e-07

 43%|████▎     | 46/108 [05:24<07:12,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4194109439849854
start make_pred
done makepred -0.007889270782470703
start save
2.384185791015625e-07

 44%|████▎     | 47/108 [05:31<07:02,  6.93s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4200239181518555
start make_pred
done makepred -0.007771492004394531
start save
4.76837158203125e-07

 44%|████▍     | 48/108 [05:38<06:57,  6.96s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4128754138946533
start make_pred
done makepred -0.0077075958251953125
start save
1.9073486328125e-06

 45%|████▌     | 49/108 [05:46<06:57,  7.07s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.410560369491577
start make_pred
done makepred -0.007851839065551758
start save
1.1920928955078125e-06

 46%|████▋     | 50/108 [05:52<06:47,  7.03s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.415844678878784
start make_pred
done makepred -0.00785517692565918
start save
1.1920928955078125e-06

 47%|████▋     | 51/108 [05:59<06:38,  6.99s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.420903205871582
start make_pred
done makepred -0.007863759994506836
start save
9.5367431640625e-07

 48%|████▊     | 52/108 [06:06<06:32,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.415884494781494
start make_pred
done makepred -0.008278846740722656
start save
7.152557373046875e-07

 49%|████▉     | 53/108 [06:13<06:25,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4165475368499756
start make_pred
done makepred -0.007762432098388672
start save
9.5367431640625e-07

 50%|█████     | 54/108 [06:20<06:17,  6.99s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4197680950164795
start make_pred
done makepred -0.007872581481933594
start save
9.5367431640625e-07

 51%|█████     | 55/108 [06:27<06:11,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4108381271362305
start make_pred
done makepred -0.007875204086303711
start save
9.5367431640625e-07

 52%|█████▏    | 56/108 [06:34<06:03,  6.98s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4202945232391357
start make_pred
done makepred -0.007819890975952148
start save
1.1920928955078125e-06

 53%|█████▎    | 57/108 [06:41<05:54,  6.96s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412736177444458
start make_pred
done makepred -0.007922887802124023
start save
7.152557373046875e-07

 54%|█████▎    | 58/108 [06:48<05:46,  6.92s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.415311813354492
start make_pred
done makepred -0.008055448532104492
start save
4.76837158203125e-07

 55%|█████▍    | 59/108 [06:55<05:42,  6.99s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.414951801300049
start make_pred
done makepred -0.007731437683105469
start save
1.430511474609375e-06

 56%|█████▌    | 60/108 [07:02<05:34,  6.97s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4158942699432373
start make_pred
done makepred -0.0078122615814208984
start save
9.5367431640625e-07

 56%|█████▋    | 61/108 [07:09<05:30,  7.03s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.415029525756836
start make_pred
done makepred -0.0077855587005615234
start save
4.76837158203125e-07

 57%|█████▋    | 62/108 [07:16<05:23,  7.03s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4140942096710205
start make_pred
done makepred -0.00737452507019043
start save
4.76837158203125e-07

 58%|█████▊    | 63/108 [07:23<05:16,  7.02s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4185779094696045
start make_pred
done makepred -0.007859230041503906
start save
4.76837158203125e-07

 59%|█████▉    | 64/108 [07:30<05:08,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4131486415863037
start make_pred
done makepred -0.0077702999114990234
start save
1.430511474609375e-06

 60%|██████    | 65/108 [07:38<05:03,  7.06s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4118597507476807
start make_pred
done makepred -0.007707118988037109
start save
7.152557373046875e-07

 61%|██████    | 66/108 [07:45<05:00,  7.16s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412503480911255
start make_pred
done makepred -0.007756471633911133
start save
4.76837158203125e-07

 62%|██████▏   | 67/108 [07:52<04:53,  7.15s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4170620441436768
start make_pred
done makepred -0.0077288150787353516
start save
7.152557373046875e-07

 63%|██████▎   | 68/108 [07:59<04:42,  7.07s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.422093629837036
start make_pred
done makepred -0.007879972457885742
start save
1.430511474609375e-06

 64%|██████▍   | 69/108 [08:06<04:34,  7.04s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.5493807792663574
start make_pred
done makepred -0.007775068283081055
start save
7.152557373046875e-07

 65%|██████▍   | 70/108 [08:13<04:27,  7.04s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4238269329071045
start make_pred
done makepred -0.007714986801147461
start save
7.152557373046875e-07

 66%|██████▌   | 71/108 [08:20<04:20,  7.03s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4144961833953857
start make_pred
done makepred -0.007665157318115234
start save
7.152557373046875e-07

 67%|██████▋   | 72/108 [08:27<04:13,  7.03s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4102835655212402
start make_pred
done makepred -0.00772857666015625
start save
7.152557373046875e-07

 68%|██████▊   | 73/108 [08:34<04:05,  7.02s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4144089221954346
start make_pred
done makepred -0.0076904296875
start save
7.152557373046875e-07

 69%|██████▊   | 74/108 [08:41<03:59,  7.05s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4142048358917236
start make_pred
done makepred -0.00780487060546875
start save
4.76837158203125e-07

 69%|██████▉   | 75/108 [08:48<03:52,  7.03s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.415280818939209
start make_pred
done makepred -0.007614612579345703
start save
4.76837158203125e-07

 70%|███████   | 76/108 [08:55<03:44,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4132659435272217
start make_pred
done makepred -0.007845163345336914
start save
1.430511474609375e-06

 71%|███████▏  | 77/108 [09:02<03:37,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.414142370223999
start make_pred
done makepred -0.007798194885253906
start save
9.5367431640625e-07

 72%|███████▏  | 78/108 [09:09<03:30,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4145686626434326
start make_pred
done makepred -0.008346319198608398
start save
4.76837158203125e-07

 73%|███████▎  | 79/108 [09:16<03:23,  7.01s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.415942430496216
start make_pred
done makepred -0.007727146148681641
start save
9.5367431640625e-07

 74%|███████▍  | 80/108 [09:23<03:18,  7.10s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.415030002593994
start make_pred
done makepred -0.008049964904785156
start save
1.1920928955078125e-06

 75%|███████▌  | 81/108 [09:31<03:14,  7.20s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.41483736038208
start make_pred
done makepred -0.007506132125854492
start save
1.430511474609375e-06

 76%|███████▌  | 82/108 [09:38<03:06,  7.16s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.415201425552368
start make_pred
done makepred -0.007740497589111328
start save
4.76837158203125e-07

 77%|███████▋  | 83/108 [09:45<02:57,  7.11s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.417306423187256
start make_pred
done makepred -0.00770258903503418
start save
1.430511474609375e-06

 78%|███████▊  | 84/108 [09:52<02:51,  7.16s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4164466857910156
start make_pred
done makepred -0.007737636566162109
start save
7.152557373046875e-07

 79%|███████▊  | 85/108 [09:59<02:43,  7.13s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4161479473114014
start make_pred
done makepred -0.007633209228515625
start save
4.76837158203125e-07

 80%|███████▉  | 86/108 [10:06<02:35,  7.09s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4139461517333984
start make_pred
done makepred -0.007711887359619141
start save
4.76837158203125e-07

 81%|████████  | 87/108 [10:13<02:28,  7.09s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4144716262817383
start make_pred
done makepred -0.007797956466674805
start save
9.5367431640625e-07

 81%|████████▏ | 88/108 [10:20<02:21,  7.10s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412409543991089
start make_pred
done makepred -0.007620573043823242
start save
1.1920928955078125e-06

 82%|████████▏ | 89/108 [10:28<02:15,  7.13s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.414135217666626
start make_pred
done makepred -0.007834672927856445
start save
1.6689300537109375e-06

 83%|████████▎ | 90/108 [10:35<02:09,  7.17s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.416100263595581
start make_pred
done makepred -0.007546186447143555
start save
1.1920928955078125e-06

 84%|████████▍ | 91/108 [10:42<02:00,  7.09s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4145519733428955
start make_pred
done makepred -0.00781559944152832
start save
9.5367431640625e-07

 85%|████████▌ | 92/108 [10:49<01:54,  7.13s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4159095287323
start make_pred
done makepred -0.008170843124389648
start save
1.430511474609375e-06

 86%|████████▌ | 93/108 [10:56<01:48,  7.22s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4123520851135254
start make_pred
done makepred -0.0077056884765625
start save
1.1920928955078125e-06

 87%|████████▋ | 94/108 [11:04<01:41,  7.25s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.415524482727051
start make_pred
done makepred -0.007712125778198242
start save
9.5367431640625e-07

 88%|████████▊ | 95/108 [11:11<01:33,  7.21s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4130613803863525
start make_pred
done makepred -0.007693290710449219
start save
7.152557373046875e-07

 89%|████████▉ | 96/108 [11:18<01:26,  7.24s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4119858741760254
start make_pred
done makepred -0.0076618194580078125
start save
2.384185791015625e-07

 90%|████████▉ | 97/108 [11:25<01:18,  7.15s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4104132652282715
start make_pred
done makepred -0.0076465606689453125
start save
9.5367431640625e-07

 91%|█████████ | 98/108 [11:32<01:11,  7.11s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4161574840545654
start make_pred
done makepred -0.007750034332275391
start save
9.5367431640625e-07

 92%|█████████▏| 99/108 [11:39<01:04,  7.16s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4157350063323975
start make_pred
done makepred -0.007654905319213867
start save
9.5367431640625e-07

 93%|█████████▎| 100/108 [11:46<00:56,  7.12s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.412886381149292
start make_pred
done makepred -0.008121490478515625
start save
7.152557373046875e-07

 94%|█████████▎| 101/108 [11:53<00:49,  7.10s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4170279502868652
start make_pred
done makepred -0.0077342987060546875
start save
7.152557373046875e-07

 94%|█████████▍| 102/108 [12:01<00:42,  7.08s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4172515869140625
start make_pred
done makepred -0.007604122161865234
start save
9.5367431640625e-07

 95%|█████████▌| 103/108 [12:08<00:35,  7.10s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.415252447128296
start make_pred
done makepred -0.0077686309814453125
start save
1.1920928955078125e-06

 96%|█████████▋| 104/108 [12:14<00:28,  7.00s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4131102561950684
start make_pred
done makepred -0.00776219367980957
start save
1.1920928955078125e-06

 97%|█████████▋| 105/108 [12:22<00:21,  7.17s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4119534492492676
start make_pred
done makepred -0.007732391357421875
start save
9.5367431640625e-07

 98%|█████████▊| 106/108 [12:29<00:14,  7.18s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4200358390808105
start make_pred
done makepred -0.00771641731262207
start save
1.1920928955078125e-06

 99%|█████████▉| 107/108 [12:36<00:07,  7.16s/it]torch.Size([3, 512, 640]) image.shape -
resize torch.Size([3, 512, 640]) to (256, 320)
torch.Size([3, 256, 320])
torch.Size([3, 256, 320]) image.shape
start pred
done pred -2.4225592613220215
start make_pred
done makepred -0.007626771926879883
start save
1.1920928955078125e-06

100%|██████████| 108/108 [12:44<00:00,  7.17s/it]
100%|██████████| 108/108 [12:44<00:00,  7.07s/it]
/scratch_net/schusch/qimaqi/miniconda3/envs/feature_3dgs/lib/python3.9/site-packages/pytorch_lightning/utilities/migration/migration.py:208: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.
Lightning automatically upgraded your loaded checkpoint from v1.3.5 to v2.2.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint demo_e200.ckpt`
** Use norm [0.5, 0.5, 0.5], [0.5, 0.5, 0.5] as the mean and std **
test rgb dir /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/.cache/images
Traceback (most recent call last):
  File "/usr/bmicnas02/data-biwi-01/qimaqi_data/workspace/neurips_2025/feature-3dgs_Qi/encoders/lseg_encoder/encode_images_matterport.py", line 585, in <module>
    test(args)
  File "/usr/bmicnas02/data-biwi-01/qimaqi_data/workspace/neurips_2025/feature-3dgs_Qi/encoders/lseg_encoder/encode_images_matterport.py", line 300, in test
    testset = get_original_dataset(
  File "/usr/bmicnas02/data-biwi-01/qimaqi_data/workspace/neurips_2025/feature-3dgs_Qi/encoders/lseg_encoder/data/__init__.py", line 139, in get_original_dataset
    assert False, f"dataset {name} not found"
AssertionError: dataset /srv/beegfs02/scratch/qimaqi_data/data/matterport_subset/.cache/images not found
